{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iust-dl97-project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOffrGEGtUcP",
        "colab_type": "text"
      },
      "source": [
        "# Train Network from Scratch for Speaker Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGyvsiqSE0U",
        "colab_type": "text"
      },
      "source": [
        "##Download and Extract VoxCeleb1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5ZqmIcGODKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\n",
        "\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\n",
        "\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data/vox1_dev_txt.zip  \n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data/vox1_test_txt.zip\n",
        "\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/iden_split.txt\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test.txt\n",
        "  \n",
        "! cat vox1_dev* > vox1_dev_wav.zip\n",
        "! rm vox1_dev_wav_partaa vox1_dev_wav_partab vox1_dev_wav_partac vox1_dev_wav_partad\n",
        "! mkdir -p voxceleb1\n",
        "! mv *.zip voxceleb1\n",
        "! mv *.txt voxceleb1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW7bg-7mSZYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "DATA_PATH = 'voxceleb1/'\n",
        "\n",
        "print('Starting to unpack vox1_dev_wav.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_dev_wav.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_test_wav.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_test_wav.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_dev_txt.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_dev_txt.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_test_txt.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_test_txt.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done.')\n",
        "\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_dev_wav.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_test_wav.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_dev_txt.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_test_txt.zip'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3myIBQcd2sb",
        "colab_type": "text"
      },
      "source": [
        "##with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzmJGsBJsWKE",
        "colab_type": "code",
        "outputId": "f18275d3-fdaf-4522-c150-a40cfa9a430c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "! pip install tensorboardX\n",
        "import tensorboardX\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otk5QT2vdLyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IdentificationDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, path, train, transform=None):\n",
        "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
        "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
        "        \n",
        "        if train:\n",
        "            phases = [1, 2]\n",
        "        \n",
        "        else:\n",
        "            phases = [3]\n",
        "            \n",
        "        mask = split['phase'].isin(phases)\n",
        "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
        "        self.path = path\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # path\n",
        "        track_path = self.dataset[idx]\n",
        "        audio_path = os.path.join(self.path, 'wav', track_path)\n",
        "\n",
        "        # read .wav\n",
        "        rate, samples = wavfile.read(audio_path)\n",
        "        # extract label from path like id10003/L9_sh8msGV59/00001.txt\n",
        "        # subtracting 1 because PyTorch assumes that C_i in [0, 1251-1]\n",
        "        label = int(track_path.split('/')[0].replace('id1', '')) - 1\n",
        "\n",
        "        ## parameters\n",
        "        window = 'hamming'\n",
        "        # window width and step size\n",
        "        Tw = 25 # ms\n",
        "        Ts = 10 # ms\n",
        "        # frame duration (samples)\n",
        "        Nw = int(rate * Tw * 1e-3)\n",
        "        Ns = int(rate * (Tw - Ts) * 1e-3)\n",
        "        # overlapped duration (samples)\n",
        "        # 2 ** to the next pow of 2 of (Nw - 1)\n",
        "        nfft = 2 ** (Nw - 1).bit_length()\n",
        "        pre_emphasis = 0.97\n",
        "        \n",
        "        # preemphasis filter\n",
        "        samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
        "        \n",
        "        # removes DC component of the signal and add a small dither\n",
        "        samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
        "        dither = np.random.uniform(-1, 1, samples.shape)\n",
        "        spow = np.std(samples)\n",
        "        samples = samples + 1e-6 * spow * dither\n",
        "        \n",
        "        if self.train:\n",
        "            # segment selection\n",
        "            segment_len = 3 # sec\n",
        "            upper_bound = len(samples) - segment_len * rate\n",
        "            start = np.random.randint(0, upper_bound)\n",
        "            end = start + segment_len * rate\n",
        "            samples = samples[start:end]\n",
        "        \n",
        "        # spectogram\n",
        "        _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                        mode='magnitude', return_onesided=False)\n",
        "        \n",
        "        # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
        "        spec *= rate / 10\n",
        "        \n",
        "        if self.transform:\n",
        "            spec = self.transform(spec)\n",
        "            \n",
        "         \n",
        "        \n",
        "        \n",
        "        \n",
        "        _, _, spec_phase = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                                mode='phase', return_onesided=False)                \n",
        "        spec_phase[1:,:] = np.diff(spec_phase, axis=0)\n",
        "        spec_phase = spec_phase.reshape(1, spec_phase.shape[0], spec_phase.shape[1])\n",
        "        spec_phase = spec_phase.astype(np.float32)\n",
        "        spec_phase = torch.from_numpy(spec_phase)\n",
        "        spec_ = np.concatenate((spec, spec_phase), axis=0)    \n",
        "        \n",
        "        \n",
        "\n",
        "        return label, spec_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yAG_7jrdPCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalize(object):\n",
        "    \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
        "    \n",
        "    def __call__(self, spec):\n",
        "        \n",
        "        # (Freq, Time)\n",
        "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
        "        mu = spec.mean(axis=1).reshape(512, 1)\n",
        "        sigma = spec.std(axis=1).reshape(512, 1)\n",
        "        spec = (spec - mu) / sigma\n",
        "\n",
        "        return spec\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert spectogram to Tensor.\"\"\"\n",
        "    \n",
        "    def __call__(self, spec):\n",
        "        F, T = spec.shape\n",
        "        \n",
        "        # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
        "        spec = spec.reshape(1, F, T)\n",
        "        \n",
        "        # make the ndarray to be of a proper type (was float64)\n",
        "        spec = spec.astype(np.float32)\n",
        "        \n",
        "        return torch.from_numpy(spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuk24rZvdSXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VoiceNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(VoiceNet, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=2, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn6 = nn.BatchNorm2d(num_features=4096)\n",
        "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
        "        \n",
        "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
        "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=4096, kernel_size=(9, 1))\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=1024)\n",
        "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.mpool1(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.mpool2(x) \n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.mpool5(x)\n",
        "        x = self.relu(self.bn6(self.fc6(x)))\n",
        "        \n",
        "        _, _, _, W = x.size()\n",
        "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
        "        x = self.apool6(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.bn7(self.fc7(x)))\n",
        "        x = self.fc8(x)\n",
        "        \n",
        "        # during training, there's no need for SoftMax because CELoss calculates it\n",
        "        if self.training:\n",
        "            return x\n",
        "        \n",
        "        else:\n",
        "            return self.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-s4Inc5dWKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_PATH = 'logs/VoxCeleb/rm_dc_n_dither'\n",
        "! mkdir -p logs/VoxCeleb/rm_dc_n_dither\n",
        "EPOCH_NUM = 30\n",
        "\n",
        "# in shared code B = 100 but PyTorch throws CUDA out of memory at B = 97 \n",
        "# though B=96 takes only 90.6% of the GPU Mem (bug?):\n",
        "# https://discuss.pytorch.org/t/lesser-memory-consumption-with-a-larger-batch-in-multi-gpu-setup/29087\n",
        "# B = 96\n",
        "# but when \n",
        "torch.backends.cudnn.deterministic = True\n",
        "# I can set B = 100\n",
        "B = 100\n",
        "\n",
        "WEIGHT_DECAY = 5e-4\n",
        "LR_INIT = 1e-2\n",
        "LR_LAST = 1e-4\n",
        "# lr scheduler parameter\n",
        "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
        "MOMENTUM = 0.9\n",
        "DEVICE = 'cuda:0'\n",
        "NUM_WORKERS = 4\n",
        "TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVTOfeZDdsDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = VoiceNet(num_classes=1251)\n",
        "net.to(DEVICE)\n",
        "\n",
        "transforms = Compose([\n",
        "    Normalize(),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "trainset = IdentificationDataset(DATASET_PATH, train=True, transform=transforms)\n",
        "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=B, num_workers=NUM_WORKERS, shuffle=True)\n",
        "\n",
        "testset = IdentificationDataset(DATASET_PATH, train=False, transform=transforms)\n",
        "testsetloader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=NUM_WORKERS*2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), LR_INIT, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzF3lwsuZR_6",
        "colab_type": "code",
        "outputId": "0c2f9f9b-fe16-49b2-e73c-e0112bcbf93e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(net, input_size=(2, 512, 300))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 96, 254, 148]           9,504\n",
            "       BatchNorm2d-2         [-1, 96, 254, 148]             192\n",
            "              ReLU-3         [-1, 96, 254, 148]               0\n",
            "         MaxPool2d-4          [-1, 96, 126, 73]               0\n",
            "            Conv2d-5          [-1, 256, 62, 36]         614,656\n",
            "       BatchNorm2d-6          [-1, 256, 62, 36]             512\n",
            "              ReLU-7          [-1, 256, 62, 36]               0\n",
            "         MaxPool2d-8          [-1, 256, 30, 17]               0\n",
            "            Conv2d-9          [-1, 256, 30, 17]         590,080\n",
            "      BatchNorm2d-10          [-1, 256, 30, 17]             512\n",
            "             ReLU-11          [-1, 256, 30, 17]               0\n",
            "           Conv2d-12          [-1, 256, 30, 17]         590,080\n",
            "      BatchNorm2d-13          [-1, 256, 30, 17]             512\n",
            "             ReLU-14          [-1, 256, 30, 17]               0\n",
            "           Conv2d-15          [-1, 256, 30, 17]         590,080\n",
            "      BatchNorm2d-16          [-1, 256, 30, 17]             512\n",
            "             ReLU-17          [-1, 256, 30, 17]               0\n",
            "        MaxPool2d-18            [-1, 256, 9, 8]               0\n",
            "           Conv2d-19           [-1, 4096, 1, 8]       9,441,280\n",
            "      BatchNorm2d-20           [-1, 4096, 1, 8]           8,192\n",
            "             ReLU-21           [-1, 4096, 1, 8]               0\n",
            "           Linear-22                 [-1, 1024]       4,195,328\n",
            "      BatchNorm1d-23                 [-1, 1024]           2,048\n",
            "             ReLU-24                 [-1, 1024]               0\n",
            "           Linear-25                 [-1, 1251]       1,282,275\n",
            "================================================================\n",
            "Total params: 17,325,763\n",
            "Trainable params: 17,325,763\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.17\n",
            "Forward/backward pass size (MB): 113.30\n",
            "Params size (MB): 66.09\n",
            "Estimated Total Size (MB): 180.56\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WjPvT8tesZZ",
        "colab_type": "code",
        "outputId": "9f2a848d-f6ef-460f-deac-9c8f35c2ac3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for epoch_num in range(EPOCH_NUM):\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # train\n",
        "    net.train()\n",
        "    \n",
        "    for iter_num, (labels, specs) in tqdm(enumerate(trainsetloader)):\n",
        "        optimizer.zero_grad()\n",
        "        labels, specs = labels.to(DEVICE), specs.to(DEVICE)\n",
        "        scores = net(specs)\n",
        "        loss = criterion(scores, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # TBoard\n",
        "        step_num = epoch_num * len(trainsetloader) + iter_num\n",
        "        TBoard.add_scalar('Metrics/train_loss', loss.item(), step_num)\n",
        "        TBoard.add_scalar('Metrics/lr', lr_scheduler.get_lr()[0], step_num)\n",
        "        \n",
        "#         TBoard.add_scalar('weights/conv1', net.conv1.weight.mean(), step_num)\n",
        "#         TBoard.add_scalar('weights/conv5', net.conv5.weight.mean(), step_num)\n",
        "#         TBoard.add_scalar('weights/fc6', net.fc6.weight.mean(), step_num)\n",
        "#         TBoard.add_scalar('weights/fc7', net.fc7.weight.mean(), step_num)\n",
        "#         TBoard.add_scalar('weights/fc8', net.fc8.weight.mean(), step_num)\n",
        "#         TBoard.add_scalar('grads/conv1', net.conv1.weight.grad.mean(), step_num)\n",
        "#         TBoard.add_scalar('grads/conv5', net.conv5.weight.grad.mean(), step_num)\n",
        "#         TBoard.add_scalar('grads/fc6', net.fc6.weight.grad.mean(), step_num)\n",
        "#         TBoard.add_scalar('grads/fc7', net.fc7.weight.grad.mean(), step_num)\n",
        "#         TBoard.add_scalar('grads/fc8', net.fc8.weight.grad.mean(), step_num)\n",
        "\n",
        "    \n",
        "    # test\n",
        "    net.eval()\n",
        "    \n",
        "    top5_accuracy = 0\n",
        "    top1_accuracy = 0\n",
        "\n",
        "    for _, (label, spec) in tqdm(enumerate(testsetloader)):\n",
        "        label, spec = label.to(DEVICE), spec.to(DEVICE)\n",
        "        probs = net(spec)\n",
        "\n",
        "        # calculate Top-5 and Top-1 accuracy\n",
        "        pred_top5 = probs.topk(5)[1].view(5)\n",
        "\n",
        "        if label in pred_top5:\n",
        "            # increment top-5 accuracy\n",
        "            top5_accuracy += 1\n",
        "\n",
        "            if label == pred_top5[0]:\n",
        "                # increment top-1 accuracy\n",
        "                top1_accuracy += 1\n",
        "\n",
        "    top5_accuracy /= len(testsetloader)\n",
        "    top1_accuracy /= len(testsetloader)\n",
        "\n",
        "    TBoard.add_scalar('Metrics/test_top5', top5_accuracy, epoch_num)\n",
        "    TBoard.add_scalar('Metrics/test_top1', top1_accuracy, epoch_num)\n",
        "    \n",
        "    print('\\ntest_top5 =', round(100 * top5_accuracy, 2), '%')\n",
        "    print('test_top1 =', round(100 * top1_accuracy, 2), '%')\n",
        "        \n",
        "# when the training is finished save the model\n",
        "torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot.txt'))\n",
        "TBoard.close()\n",
        "print('top 1 accuracy @ the end: {}'.format(round(top1_accuracy, 3)))\n",
        "print('top 5 accuracy @ the end: {}'.format(round(top5_accuracy, 3)))\n",
        "print('loss @ the end: {}'.format(round(loss.item(), 3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1453it [1:13:42,  1.81s/it]\n",
            "8251it [10:21, 13.27it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test_top5 = 3.56 %\n",
            "test_top1 = 3.56 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "526it [26:55,  3.18s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3BLgttW9DZm",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Trained Model as Feature Extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrpJ4Of13y4T",
        "colab_type": "text"
      },
      "source": [
        "##with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwUZ939VnXVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8e422a8b-f1fc-4bb2-9297-66ae544a1134"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPl_qabGK_0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-3/fa.tar.gz\n",
        "# ! cp fa.tar.gz drive/My\\ Drive/datasets/fa.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apjtPrqY0nOO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a6a21e61-a0cc-4f74-871f-6ae6eb18a85f"
      },
      "source": [
        "! cp drive/My\\ Drive/datasets/fa.tar.gz fa.tar.gz\n",
        "! mkdir common_voice\n",
        "! tar -C common_voice -xf fa.tar.gz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘common_voice’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RruP61bg1V5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('common_voice/validated.tsv', 'r') as val:\n",
        "  lines = val.readlines()\n",
        "  \n",
        "clients_id = []\n",
        "files_name = []\n",
        "mp3_name = []\n",
        "for x in lines[1:]:\n",
        "  clients_id.append(x.split()[0])\n",
        "  files_name.append(x.split()[1].replace('mp3','wav'))\n",
        "  mp3_name.append(x.split()[1])\n",
        "  \n",
        "import collections\n",
        "sps = 2\n",
        "spk_id = [item for item, count in collections.Counter(clients_id).items() if count >= sps]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5pbNhZn2EQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "spk_index = []\n",
        "file_index = []\n",
        "mp3_index = []\n",
        "for i, sid in enumerate(spk_id):\n",
        "  idx = clients_id.index(sid)\n",
        "  [spk_index.append(i) for f in clients_id[idx : idx+sps]]\n",
        "  [file_index.append(os.path.join(DATA_PATH, f)) for f in files_name[idx : idx+sps]]\n",
        "  [mp3_index.append(os.path.join('common_voice/clips', f)) for f in mp3_name[idx : idx+sps]]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKvHmqB9LNYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# mp3 to wav\n",
        "os.mkdir('common_voice/wav')\n",
        "for i, wav in enumerate(file_index):\n",
        "  os.system('ffmpeg -i {} -ar 16000 {}'.format(mp3_index[i], wav))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST9OCSDqrSg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/mohsenoon/iust-dl97-project.git\n",
        "! apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0\n",
        "! pip install pyaudio\n",
        "# % cd vggvox-speaker-identification/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daU9GgWNMl3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('iust-dl97-project/cfg/enroll_list.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['filename', 'speaker']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in range(0, len(file_index)-2000, 2):\n",
        "      writer.writerow({'filename': '../'+file_index[i], 'speaker': spk_index[i]})\n",
        "\n",
        "with open('iust-dl97-project/cfg/test_list.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['filename', 'speaker']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in range(1, len(file_index)-2000, 2):\n",
        "      writer.writerow({'filename': '../'+file_index[i], 'speaker': spk_index[i]})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Cyy2OvrlA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b675506a-c73a-4d54-8981-23b84d06f6c7"
      },
      "source": [
        "% cd iust-dl97-project\n",
        "! python scoring.py\n",
        "% cd .."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/vggvox-speaker-identification\n",
            "Using TensorFlow backend.\n",
            "Loading model weights from [data/model/weights.h5]....\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 13:33:59.695988 140664499038080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 13:33:59.712968 140664499038080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 13:33:59.741338 140664499038080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 13:33:59.741575 140664499038080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0721 13:33:59.741755 140664499038080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2019-07-21 13:33:59.747425: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-21 13:33:59.747689: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x57128c0 executing computations on platform Host. Devices:\n",
            "2019-07-21 13:33:59.747728: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-21 13:33:59.750000: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-21 13:33:59.855949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 13:33:59.856567: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5712c40 executing computations on platform CUDA. Devices:\n",
            "2019-07-21 13:33:59.856604: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-07-21 13:33:59.856878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 13:33:59.857324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-21 13:33:59.857726: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 13:33:59.859421: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-21 13:33:59.861145: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-21 13:33:59.861587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-21 13:33:59.863856: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-21 13:33:59.865677: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-21 13:33:59.870295: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-21 13:33:59.870455: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 13:33:59.870928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 13:33:59.871316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-21 13:33:59.871417: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 13:33:59.872967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-21 13:33:59.873003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-21 13:33:59.873020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-21 13:33:59.873401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 13:33:59.873859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 13:33:59.874229: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-21 13:33:59.874329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0721 13:34:00.351963 140664499038080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0721 13:34:00.431785 140664499038080 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 512, None, 1)      0         \n",
            "_________________________________________________________________\n",
            "pad1 (ZeroPadding2D)         (None, 514, None, 1)      0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 254, None, 96)     4800      \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 254, None, 96)     384       \n",
            "_________________________________________________________________\n",
            "relu1 (Activation)           (None, 254, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "mpool1 (MaxPooling2D)        (None, 126, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "pad2 (ZeroPadding2D)         (None, 128, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 62, None, 256)     614656    \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 62, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu2 (Activation)           (None, 62, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "mpool2 (MaxPooling2D)        (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "pad3 (ZeroPadding2D)         (None, 32, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 30, None, 384)     885120    \n",
            "_________________________________________________________________\n",
            "bn3 (BatchNormalization)     (None, 30, None, 384)     1536      \n",
            "_________________________________________________________________\n",
            "relu3 (Activation)           (None, 30, None, 384)     0         \n",
            "_________________________________________________________________\n",
            "pad4 (ZeroPadding2D)         (None, 32, None, 384)     0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 30, None, 256)     884992    \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 30, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu4 (Activation)           (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "pad5 (ZeroPadding2D)         (None, 32, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 30, None, 256)     590080    \n",
            "_________________________________________________________________\n",
            "bn5 (BatchNormalization)     (None, 30, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu5 (Activation)           (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "mpool5 (MaxPooling2D)        (None, 9, None, 256)      0         \n",
            "_________________________________________________________________\n",
            "pad6 (ZeroPadding2D)         (None, 9, None, 256)      0         \n",
            "_________________________________________________________________\n",
            "fc6 (Conv2D)                 (None, 1, None, 4096)     9441280   \n",
            "_________________________________________________________________\n",
            "bn6 (BatchNormalization)     (None, 1, None, 4096)     16384     \n",
            "_________________________________________________________________\n",
            "relu6 (Activation)           (None, 1, None, 4096)     0         \n",
            "_________________________________________________________________\n",
            "gapool6 (GlobalAveragePoolin (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape6 (Reshape)           (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "pad7 (ZeroPadding2D)         (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "fc7 (Conv2D)                 (None, 1, 1, 1024)        4195328   \n",
            "_________________________________________________________________\n",
            "bn7 (BatchNormalization)     (None, 1, 1, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "relu7 (Activation)           (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "norm (Lambda)                (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "fc8 (Conv2D)                 (None, 1, 1, 1024)        1049600   \n",
            "=================================================================\n",
            "Total params: 17,691,328\n",
            "Trainable params: 17,678,592\n",
            "Non-trainable params: 12,736\n",
            "_________________________________________________________________\n",
            "Processing enroll samples....\n",
            "Traceback (most recent call last):\n",
            "  File \"scoring.py\", line 85, in <module>\n",
            "    get_id_result()\n",
            "  File \"scoring.py\", line 59, in get_id_result\n",
            "    enroll_result = get_embeddings_from_list_file(model, c.ENROLL_LIST_FILE, c.MAX_SEC)\n",
            "  File \"scoring.py\", line 47, in get_embeddings_from_list_file\n",
            "    result['features'] = result['filename'].apply(lambda x: get_fft_spectrum(x, buckets))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\", line 3591, in apply\n",
            "    mapped = lib.map_infer(values, f, convert=convert_dtype)\n",
            "  File \"pandas/_libs/lib.pyx\", line 2217, in pandas._libs.lib.map_infer\n",
            "  File \"scoring.py\", line 47, in <lambda>\n",
            "    result['features'] = result['filename'].apply(lambda x: get_fft_spectrum(x, buckets))\n",
            "  File \"/content/vggvox-speaker-identification/wav_reader.py\", line 36, in get_fft_spectrum\n",
            "    signal = load_wav(filename,c.SAMPLE_RATE)\n",
            "  File \"/content/vggvox-speaker-identification/wav_reader.py\", line 10, in load_wav\n",
            "    audio, sr = librosa.load(filename, sr=sample_rate, mono=True)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/librosa/core/audio.py\", line 119, in load\n",
            "    with audioread.audio_open(os.path.realpath(path)) as input_file:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/audioread/__init__.py\", line 111, in audio_open\n",
            "    return BackendClass(path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/audioread/rawread.py\", line 62, in __init__\n",
            "    self._fh = open(filename, 'rb')\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/common_voice/wav/common_voice_fa_18582295.wav'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXhSIZEv8uOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}