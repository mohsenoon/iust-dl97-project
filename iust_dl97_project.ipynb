{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iust-dl97-project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "eBglY0n5AVht",
        "fEGyvsiqSE0U",
        "K3myIBQcd2sb",
        "ukgvL1wd6cIZ",
        "jalpyzbF_Coz",
        "uvWx0xYa_K-B",
        "UWIMNoKR_Omk",
        "KXsVV5Tz_cwD",
        "BtRZxXJ0_mVI",
        "phjIuS1i_XSc",
        "enwTgQnu_rek",
        "QrTLqGApdGmU",
        "jU8gCioKAah4",
        "VAhpWV9WdvX4",
        "euroBGz_dstG",
        "zd3fxoxSd0g7",
        "TpMBJ_fM240F",
        "GKbfxyJ__pEG",
        "S66ghjND_2MU",
        "mt6lJ5AOLWGY",
        "6_kmm5jgd3-J",
        "xSfJpJd5d8_Q",
        "ydvybnzSABuL",
        "pMTy4v4d97n1",
        "2ZVA0Qxc-ICS",
        "fyQQsT5_AIUo",
        "7_-qyaGJvjw4",
        "Np6v9QgEQ0t6",
        "CWbRl_3XQ8M6",
        "qrpJ4Of13y4T",
        "Ik3u2xc_7JhB",
        "N21nTJbQ-m9-",
        "nzr_hqSm7bnK",
        "wWdrSB259WPv",
        "P1zyvd83nnsZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOffrGEGtUcP",
        "colab_type": "text"
      },
      "source": [
        "# Train Network from Scratch for Speaker Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBglY0n5AVht",
        "colab_type": "text"
      },
      "source": [
        "##Mount Google Drive as needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqkda9chpLhA",
        "colab_type": "code",
        "outputId": "dac80d95-55a2-4bdd-dc44-313b8355dfa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGyvsiqSE0U",
        "colab_type": "text"
      },
      "source": [
        "##Download and Extract VoxCeleb1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5ZqmIcGODKf",
        "colab_type": "code",
        "outputId": "7f93617d-c42d-4b44-a363-33deaac669c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\n",
        "\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\n",
        "\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data/vox1_dev_txt.zip  \n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data/vox1_test_txt.zip\n",
        "\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/iden_split.txt\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test.txt\n",
        "  \n",
        "! cat vox1_dev* > vox1_dev_wav.zip\n",
        "! rm vox1_dev_wav_partaa vox1_dev_wav_partab vox1_dev_wav_partac vox1_dev_wav_partad\n",
        "! mkdir -p voxceleb1\n",
        "! mv *.zip voxceleb1\n",
        "! mv *.txt voxceleb1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-24 20:29:59--  http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 401 Unauthorized\n",
            "Authentication selected: Basic realm=\"VoxCeleb1\"\n",
            "Reusing existing connection to www.robots.ox.ac.uk:80.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10737418240 (10G)\n",
            "Saving to: ‘vox1_dev_wav_partaa’\n",
            "\n",
            "vox1_dev_wav_partaa   0%[                    ]  56.41M  9.14MB/s    eta 15m 46s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW7bg-7mSZYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "DATA_PATH = 'voxceleb1/'\n",
        "\n",
        "print('Starting to unpack vox1_dev_wav.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_dev_wav.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_test_wav.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_test_wav.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_dev_txt.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_dev_txt.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_test_txt.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_test_txt.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done.')\n",
        "\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_dev_wav.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_test_wav.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_dev_txt.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_test_txt.zip'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3myIBQcd2sb",
        "colab_type": "text"
      },
      "source": [
        "##VGG-M with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukgvL1wd6cIZ",
        "colab_type": "text"
      },
      "source": [
        "###Import Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzmJGsBJsWKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import Compose\n",
        "\n",
        "# ! pip install tensorboardX\n",
        "# import tensorboardX\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)\n",
        "\n",
        "DATA_PATH = 'voxceleb1/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jalpyzbF_Coz",
        "colab_type": "text"
      },
      "source": [
        "###Read wav files and calculate spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Otk5QT2vdLyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class IdentificationDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, path, train, transform=None):\n",
        "        iden_split_path = os.path.join(path, 'iden_split.txt')\n",
        "        split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
        "        \n",
        "        if train=='train':\n",
        "            phases = [1]\n",
        "        elif train=='valid':\n",
        "            phases = [2]\n",
        "        else:\n",
        "            phases = [3]\n",
        "        \n",
        "        mask = split['phase'].isin(phases)\n",
        "        self.dataset = split['path'][mask].reset_index(drop=True)\n",
        "        self.dataset = self.dataset[:int(len(self.dataset)/2)]\n",
        "        self.path = path\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # path\n",
        "        track_path = self.dataset[idx]\n",
        "        audio_path = os.path.join(self.path, 'wav', track_path)\n",
        "\n",
        "        # read .wav\n",
        "        rate, samples = wavfile.read(audio_path)\n",
        "        # extract label from path like id10003/L9_sh8msGV59/00001.txt\n",
        "        # subtracting 1 because PyTorch assumes that C_i in [0, 1251-1]\n",
        "        label = int(track_path.split('/')[0].replace('id1', '')) - 1\n",
        "\n",
        "        ## parameters\n",
        "        window = 'hamming'\n",
        "        # window width and step size\n",
        "        Tw = 25 # ms\n",
        "        Ts = 10 # ms\n",
        "        # frame duration (samples)\n",
        "        Nw = int(rate * Tw * 1e-3)\n",
        "        Ns = int(rate * (Tw - Ts) * 1e-3)\n",
        "        # overlapped duration (samples)\n",
        "        # 2 ** to the next pow of 2 of (Nw - 1)\n",
        "        nfft = 2 ** (Nw - 1).bit_length()\n",
        "        pre_emphasis = 0.97\n",
        "        \n",
        "        # preemphasis filter\n",
        "        samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
        "        \n",
        "        # removes DC component of the signal and add a small dither\n",
        "        samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
        "        dither = np.random.uniform(-1, 1, samples.shape)\n",
        "        spow = np.std(samples)\n",
        "        samples = samples + 1e-6 * spow * dither\n",
        "        \n",
        "        if self.train == 'train':\n",
        "            # segment selection\n",
        "            segment_len = 3 # sec\n",
        "            upper_bound = len(samples) - segment_len * rate\n",
        "            start = np.random.randint(0, upper_bound)\n",
        "            end = start + segment_len * rate\n",
        "            samples = samples[start:end]\n",
        "        \n",
        "        # spectogram\n",
        "        _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                        mode='magnitude', return_onesided=False)\n",
        "        \n",
        "        # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
        "        spec *= rate / 10\n",
        "        \n",
        "        if self.transform:\n",
        "            spec = self.transform(spec)\n",
        "            \n",
        "         \n",
        "        \n",
        "        \n",
        "        \n",
        "#         _, _, spec_phase = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "#                                                 mode='phase', return_onesided=False)                \n",
        "#         spec_phase[1:,:] = np.diff(spec_phase, axis=0)\n",
        "#         spec_phase = spec_phase.reshape(1, spec_phase.shape[0], spec_phase.shape[1])\n",
        "#         spec_phase = spec_phase.astype(np.float32)\n",
        "#         spec_phase = torch.from_numpy(spec_phase)\n",
        "#         spec_ = np.concatenate((spec, spec_phase), axis=0)    \n",
        "        \n",
        "        \n",
        "\n",
        "        return label, spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvWx0xYa_K-B",
        "colab_type": "text"
      },
      "source": [
        "###mean and variance normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yAG_7jrdPCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Normalize(object):\n",
        "    \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
        "    \n",
        "    def __call__(self, spec):\n",
        "        \n",
        "        # (Freq, Time)\n",
        "        # mean-variance normalization for every spectrogram (not batch-wise)\n",
        "        mu = spec.mean(axis=1).reshape(512, 1)\n",
        "        sigma = spec.std(axis=1).reshape(512, 1)\n",
        "        spec = (spec - mu) / sigma\n",
        "\n",
        "        return spec\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert spectogram to Tensor.\"\"\"\n",
        "    \n",
        "    def __call__(self, spec):\n",
        "        F, T = spec.shape\n",
        "        \n",
        "        # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
        "        spec = spec.reshape(1, F, T)\n",
        "        \n",
        "        # make the ndarray to be of a proper type (was float64)\n",
        "        spec = spec.astype(np.float32)\n",
        "        \n",
        "        return torch.from_numpy(spec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWIMNoKR_Omk",
        "colab_type": "text"
      },
      "source": [
        "###create model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuk24rZvdSXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VoiceNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(VoiceNet, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=96, kernel_size=7, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
        "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn3 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
        "        self.bn6 = nn.BatchNorm2d(num_features=2048)\n",
        "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.mpool5 = nn.MaxPool2d(kernel_size=(5, 3), stride=(3, 2))\n",
        "        \n",
        "        # Conv2d with weights of size (H, 1) is identical to FC with H weights\n",
        "        self.fc6 = nn.Conv2d(in_channels=256, out_channels=2048, kernel_size=(9, 1))\n",
        "        self.fc7 = nn.Linear(in_features=2048, out_features=1024)\n",
        "        self.fc8 = nn.Linear(in_features=1024, out_features=num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.mpool1(x)\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.mpool2(x) \n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.mpool5(x)\n",
        "        x = self.relu(self.bn6(self.fc6(x)))\n",
        "        \n",
        "        _, _, _, W = x.size()\n",
        "        self.apool6 = nn.AvgPool2d(kernel_size=(1, W))\n",
        "        x = self.apool6(x)\n",
        "        \n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.relu(self.bn7(self.fc7(x)))\n",
        "        x = self.fc8(x)\n",
        "        \n",
        "        # during training, there's no need for SoftMax because CELoss calculates it\n",
        "        if self.training:\n",
        "            return x\n",
        "        \n",
        "        else:\n",
        "            return self.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXsVV5Tz_cwD",
        "colab_type": "text"
      },
      "source": [
        "###set hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-s4Inc5dWKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_PATH = 'logs/VoxCeleb/rm_dc_n_dither'\n",
        "! mkdir -p logs/VoxCeleb/rm_dc_n_dither\n",
        "EPOCH_NUM = 10 #30\n",
        "\n",
        "# in shared code B = 100 but PyTorch throws CUDA out of memory at B = 97 \n",
        "# though B=96 takes only 90.6% of the GPU Mem (bug?):\n",
        "# https://discuss.pytorch.org/t/lesser-memory-consumption-with-a-larger-batch-in-multi-gpu-setup/29087\n",
        "# B = 96\n",
        "# but when \n",
        "torch.backends.cudnn.deterministic = True\n",
        "# I can set B = 100\n",
        "B = 100\n",
        "\n",
        "WEIGHT_DECAY = 5e-4\n",
        "LR_INIT = 1e-2\n",
        "LR_LAST = 1e-4\n",
        "# lr scheduler parameter\n",
        "gamma = 10 ** (np.log10(LR_LAST / LR_INIT) / (EPOCH_NUM - 1))\n",
        "MOMENTUM = 0.9\n",
        "DEVICE = 'cuda:0'\n",
        "NUM_WORKERS = 4\n",
        "# TBoard = tensorboardX.SummaryWriter(log_dir=LOG_PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtRZxXJ0_mVI",
        "colab_type": "text"
      },
      "source": [
        "###create model and data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVTOfeZDdsDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = VoiceNet(num_classes=1251)\n",
        "net.to(DEVICE)\n",
        "\n",
        "transforms = Compose([\n",
        "    Normalize(),\n",
        "    ToTensor()\n",
        "])\n",
        "\n",
        "trainset = IdentificationDataset(DATA_PATH, train='train', transform=transforms)\n",
        "trainsetloader = torch.utils.data.DataLoader(trainset, batch_size=B, num_workers=NUM_WORKERS, shuffle=True)\n",
        "\n",
        "testset = IdentificationDataset(DATA_PATH, train='test', transform=transforms)\n",
        "testsetloader = torch.utils.data.DataLoader(testset, batch_size=1, num_workers=NUM_WORKERS*2)\n",
        "\n",
        "validset = IdentificationDataset(DATA_PATH, train='valid', transform=transforms)\n",
        "validsetloader = torch.utils.data.DataLoader(validset, batch_size=1, num_workers=NUM_WORKERS*2)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), LR_INIT, MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=gamma)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjIuS1i_XSc",
        "colab_type": "text"
      },
      "source": [
        "###model info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzF3lwsuZR_6",
        "colab_type": "code",
        "outputId": "930ea6fd-8f15-4f22-9d6c-d015acf0d593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(net, input_size=(1, 512, 300))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 96, 254, 148]           4,800\n",
            "       BatchNorm2d-2         [-1, 96, 254, 148]             192\n",
            "              ReLU-3         [-1, 96, 254, 148]               0\n",
            "         MaxPool2d-4          [-1, 96, 126, 73]               0\n",
            "            Conv2d-5          [-1, 256, 62, 36]         614,656\n",
            "       BatchNorm2d-6          [-1, 256, 62, 36]             512\n",
            "              ReLU-7          [-1, 256, 62, 36]               0\n",
            "         MaxPool2d-8          [-1, 256, 30, 17]               0\n",
            "            Conv2d-9          [-1, 256, 30, 17]         590,080\n",
            "      BatchNorm2d-10          [-1, 256, 30, 17]             512\n",
            "             ReLU-11          [-1, 256, 30, 17]               0\n",
            "           Conv2d-12          [-1, 256, 30, 17]         590,080\n",
            "      BatchNorm2d-13          [-1, 256, 30, 17]             512\n",
            "             ReLU-14          [-1, 256, 30, 17]               0\n",
            "           Conv2d-15          [-1, 256, 30, 17]         590,080\n",
            "      BatchNorm2d-16          [-1, 256, 30, 17]             512\n",
            "             ReLU-17          [-1, 256, 30, 17]               0\n",
            "        MaxPool2d-18            [-1, 256, 9, 8]               0\n",
            "           Conv2d-19           [-1, 2048, 1, 8]       4,720,640\n",
            "      BatchNorm2d-20           [-1, 2048, 1, 8]           4,096\n",
            "             ReLU-21           [-1, 2048, 1, 8]               0\n",
            "           Linear-22                 [-1, 1024]       2,098,176\n",
            "      BatchNorm1d-23                 [-1, 1024]           2,048\n",
            "             ReLU-24                 [-1, 1024]               0\n",
            "           Linear-25                 [-1, 1251]       1,282,275\n",
            "================================================================\n",
            "Total params: 10,499,171\n",
            "Trainable params: 10,499,171\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.59\n",
            "Forward/backward pass size (MB): 112.92\n",
            "Params size (MB): 40.05\n",
            "Estimated Total Size (MB): 153.56\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enwTgQnu_rek",
        "colab_type": "text"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WjPvT8tesZZ",
        "colab_type": "code",
        "outputId": "148a7173-4139-49ba-e960-a9d1fe8f1dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "for epoch_num in range(EPOCH_NUM):\n",
        "    lr_scheduler.step()\n",
        "    \n",
        "    # train\n",
        "    net.train()\n",
        "    \n",
        "    for iter_num, (labels, specs) in tqdm(enumerate(trainsetloader)):\n",
        "        optimizer.zero_grad()\n",
        "        labels, specs = labels.to(DEVICE), specs.to(DEVICE)\n",
        "        scores = net(specs)\n",
        "        loss = criterion(scores, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    \n",
        "    # validation\n",
        "    net.eval()\n",
        "    \n",
        "    top5_accuracy = 0\n",
        "    top1_accuracy = 0\n",
        "\n",
        "    for _, (label, spec) in tqdm(enumerate(validsetloader)):\n",
        "        label, spec = label.to(DEVICE), spec.to(DEVICE)\n",
        "        probs = net(spec)\n",
        "\n",
        "        # calculate Top-5 and Top-1 accuracy\n",
        "        pred_top5 = probs.topk(5)[1].view(5)\n",
        "\n",
        "        if label in pred_top5:\n",
        "            # increment top-5 accuracy\n",
        "            top5_accuracy += 1\n",
        "\n",
        "            if label == pred_top5[0]:\n",
        "                # increment top-1 accuracy\n",
        "                top1_accuracy += 1\n",
        "\n",
        "    top5_accuracy /= len(validsetloader)\n",
        "    top1_accuracy /= len(validsetloader)\n",
        "\n",
        "    \n",
        "    print('\\nvalidation_top5 =', round(100 * top5_accuracy, 2), '%')\n",
        "    print('validation_top1 =', round(100 * top1_accuracy, 2), '%')\n",
        "    \n",
        "    torch.save(net, 'vggm_torch.pt')\n",
        "        \n",
        "# when the training is finished save the model\n",
        "# torch.save(net.state_dict(), os.path.join(LOG_PATH, 'model_snapshot.txt'))\n",
        "os.system('cp vggm_torch.pt drive/My\\ Drive/models/vggm_torch.pt')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "692it [19:23,  1.66s/it]\n",
            "3452it [01:49, 31.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 13.3 %\n",
            "validation_top1 = 4.32 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:20,  1.66s/it]\n",
            "3452it [01:47, 32.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 24.13 %\n",
            "validation_top1 = 9.04 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:21,  1.66s/it]\n",
            "3452it [01:48, 31.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 34.13 %\n",
            "validation_top1 = 16.02 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:23,  1.67s/it]\n",
            "3452it [01:49, 31.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 41.57 %\n",
            "validation_top1 = 20.83 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:24,  1.66s/it]\n",
            "3452it [01:48, 31.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 43.8 %\n",
            "validation_top1 = 22.54 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:22,  1.66s/it]\n",
            "3452it [01:48, 31.71it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 45.6 %\n",
            "validation_top1 = 25.49 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:23,  1.67s/it]\n",
            "3452it [01:49, 31.63it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 47.54 %\n",
            "validation_top1 = 26.39 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:21,  1.66s/it]\n",
            "3452it [01:48, 31.92it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 48.2 %\n",
            "validation_top1 = 27.26 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:22,  1.67s/it]\n",
            "3452it [01:49, 31.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 48.9 %\n",
            "validation_top1 = 27.78 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "692it [19:24,  1.66s/it]\n",
            "3452it [01:49, 31.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "validation_top5 = 49.25 %\n",
            "validation_top1 = 28.39 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrTLqGApdGmU",
        "colab_type": "text"
      },
      "source": [
        "###Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VSooYxXc-tM",
        "colab_type": "code",
        "outputId": "77d5ef2b-c994-4933-c36f-99f747f2ae22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "net.eval()\n",
        "\n",
        "top5_accuracy = 0\n",
        "top1_accuracy = 0\n",
        "\n",
        "for _, (label, spec) in tqdm(enumerate(testsetloader)):\n",
        "    label, spec = label.to(DEVICE), spec.to(DEVICE)\n",
        "    probs = net(spec)\n",
        "\n",
        "    # calculate Top-5 and Top-1 accuracy\n",
        "    pred_top5 = probs.topk(5)[1].view(5)\n",
        "\n",
        "    if label in pred_top5:\n",
        "        # increment top-5 accuracy\n",
        "        top5_accuracy += 1\n",
        "\n",
        "        if label == pred_top5[0]:\n",
        "            # increment top-1 accuracy\n",
        "            top1_accuracy += 1\n",
        "\n",
        "top5_accuracy /= len(testsetloader)\n",
        "top1_accuracy /= len(testsetloader)\n",
        "\n",
        "print('\\ntest_top5 =', round(100 * top5_accuracy, 2), '%')\n",
        "print('test_top1 =', round(100 * top1_accuracy, 2), '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4125it [02:13, 30.83it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "test_top5 = 48.75 %\n",
            "test_top1 = 27.18 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVT6tCebaBiV",
        "colab_type": "text"
      },
      "source": [
        "##VGG-M and ResNet50 with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU8gCioKAah4",
        "colab_type": "text"
      },
      "source": [
        "###Import Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MBde_yRcstI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)\n",
        "\n",
        "DATA_PATH = 'voxceleb1/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAhpWV9WdvX4",
        "colab_type": "text"
      },
      "source": [
        "###Data Generator (function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q18XcCk2dFsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, train=True, batch_size=32, dim=(512,300), n_channels=1,\n",
        "                 n_classes=1251, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.train = train\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        labels_temp = [self.labels[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp, labels_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        if self.train:\n",
        "          X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, audio_path in enumerate(list_IDs_temp):          \n",
        "          # Store class\n",
        "          y[i] = labels_temp[i]\n",
        "\n",
        "          # read .wav\n",
        "          rate, samples = wavfile.read(audio_path)\n",
        "          \n",
        "          ## parameters\n",
        "          window = 'hamming'\n",
        "          # window width and step size\n",
        "          Tw = 25 # ms\n",
        "          Ts = 10 # ms\n",
        "          # frame duration (samples)\n",
        "          Nw = int(rate * Tw * 1e-3)\n",
        "          Ns = int(rate * (Tw - Ts) * 1e-3)\n",
        "          # overlapped duration (samples)\n",
        "          # 2 ** to the next pow of 2 of (Nw - 1)\n",
        "          nfft = 2 ** (Nw - 1).bit_length()\n",
        "          pre_emphasis = 0.97\n",
        "\n",
        "          # preemphasis filter\n",
        "          samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
        "\n",
        "          # removes DC component of the signal and add a small dither\n",
        "          samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
        "          dither = np.random.uniform(-1, 1, samples.shape)\n",
        "          spow = np.std(samples)\n",
        "          samples = samples + 1e-6 * spow * dither\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          for _ in range(2):\n",
        "            samples = np.append(samples, samples)\n",
        "         \n",
        "        \n",
        "        \n",
        "          if self.train:\n",
        "          # segment selection\n",
        "            segment_len = 3 # sec\n",
        "            upper_bound = len(samples) - segment_len * rate\n",
        "            start = np.random.randint(0, upper_bound)\n",
        "            end = start + segment_len * rate\n",
        "            samples = samples[start:end]  \n",
        "\n",
        "          # spectogram\n",
        "          _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                          mode='magnitude', return_onesided=False)\n",
        "          \n",
        "          if not self.train:\n",
        "            X = np.empty((self.batch_size, spec.shape[0], spec.shape[1], self.n_channels))\n",
        "\n",
        "          # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
        "          spec *= rate / 10\n",
        "          \n",
        "          spec = Normalize(spec)\n",
        "          spec = ToTensor(spec)\n",
        "          \n",
        "          # Store sample\n",
        "          X[i,] = spec\n",
        "        \n",
        "        \n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euroBGz_dstG",
        "colab_type": "text"
      },
      "source": [
        "###Normalization (function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckk8EkVMcuEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalize(spec):\n",
        "  \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
        "  # (Freq, Time)\n",
        "  # mean-variance normalization for every spectrogram (not batch-wise)\n",
        "  mu = spec.mean(axis=1).reshape(spec.shape[0], 1)\n",
        "  sigma = spec.std(axis=1).reshape(spec.shape[0], 1)\n",
        "  spec = (spec - mu) / sigma\n",
        "\n",
        "  return spec\n",
        "\n",
        "def ToTensor(spec):\n",
        "  \"\"\"Convert spectogram to Tensor.\"\"\"\n",
        "  F, T = spec.shape\n",
        "\n",
        "  # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
        "  spec = spec.reshape(F, T, 1)\n",
        "\n",
        "  # make the ndarray to be of a proper type (was float64)\n",
        "  spec = spec.astype(np.float32)\n",
        "\n",
        "  return spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd3fxoxSd0g7",
        "colab_type": "text"
      },
      "source": [
        "###Split Data (function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx_Cz5kmdKr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(phase):\n",
        "  iden_split_path = os.path.join(DATA_PATH, 'iden_split.txt')\n",
        "  split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
        "\n",
        "  if phase == 'train':\n",
        "    phases = [1,2]\n",
        "#   elif phase == 'vaid':\n",
        "#     phases = [2]\n",
        "  else:\n",
        "    phases = [3]\n",
        "\n",
        "  mask = split['phase'].isin(phases)\n",
        "\n",
        "  dataset = split['path'][mask].reset_index(drop=True)\n",
        "  path = DATA_PATH\n",
        "\n",
        "  list_IDs = [os.path.join(DATA_PATH, 'wav', track_path) for track_path in dataset]\n",
        "  labels = [int(track_path.split('/')[0].replace('id1', '')) - 1 for track_path in dataset]\n",
        "  \n",
        "  return list_IDs[:int(len(list_IDs)/2)], labels[:int(len(list_IDs)/2)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE7RY9UR-SrT",
        "colab_type": "text"
      },
      "source": [
        "###Vgg-M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMBJ_fM240F",
        "colab_type": "text"
      },
      "source": [
        "####Model Definition - VGG-M  (function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9bI2hYy28NP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8981eb95-a4b4-4e94-ed7f-b05ae52f59e6"
      },
      "source": [
        "from keras import Model\n",
        "from keras.layers import Conv2D, Dense, Flatten, Activation, Input, Reshape\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def vgg_m_builder(num_classes, num_channel):\n",
        "  \n",
        "  inputs = Input(shape=(None,None,num_channel), name='input')\n",
        "\n",
        "  x = Conv2D(96, 7, strides=2, padding='same', name='conv1')(inputs)\n",
        "  x = BatchNormalization(trainable=False, name='batch1')(x)\n",
        "  x = Activation('relu', name='act1')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool1')(x)\n",
        "  \n",
        "  x = Conv2D(256, 5, strides=2, padding='same', name='conv2')(x)\n",
        "  x = BatchNormalization(name='batch2')(x)\n",
        "  x = Activation('relu', name='act2')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool2')(x)\n",
        "  \n",
        "  x = Conv2D(384, 3, strides=1, padding='same', name='conv3')(x)\n",
        "  x = BatchNormalization(name='batch3')(x)\n",
        "  x = Activation('relu', name='act3')(x)\n",
        "  \n",
        "  x = Conv2D(256, 3, strides=1, padding='same', name='conv4')(x)\n",
        "  x = BatchNormalization(name='batch4')(x)\n",
        "  x = Activation('relu', name='act4')(x)\n",
        "  \n",
        "  x = Conv2D(256, 3, strides=1, padding='same', name='conv5')(x)\n",
        "  x = BatchNormalization(name='batch5')(x)\n",
        "  x = Activation('relu', name='act5')(x)\n",
        "  x = MaxPooling2D(pool_size=(5,3), strides=(3,2), name='mpool5')(x)\n",
        "  \n",
        "  x = Conv2D(2048, (9,1), strides=1, padding='valid', name='fc6')(x)\n",
        "  x = BatchNormalization(name='batch6')(x)\n",
        "  x = Activation('relu', name='act6')(x)\n",
        "#   x = AveragePooling2D(pool_size=(1,int(x.shape[2])), strides=1, name='apool6')(x)\n",
        "#   x = Flatten(name='flat1')(x)\n",
        "\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  \n",
        "#   x = Reshape((1,1,4096))(x)\n",
        "  \n",
        "  x = Dense(1024, name='fc7')(x)\n",
        "  x = BatchNormalization(name='batch7')(x)\n",
        "  x = Activation('relu', name='act7')(x)    \n",
        "  \n",
        "  predictions = Dense(num_classes, activation='softmax', name='fc8')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKbfxyJ__pEG",
        "colab_type": "text"
      },
      "source": [
        "####Build and Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4hsJnhd-d76",
        "colab_type": "code",
        "outputId": "35d77eb1-a3df-42c9-bbee-ad6a3cd959bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = vgg_m_builder(num_classes=1251, num_channel=1)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, None, None, 1)     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, None, None, 96)    4800      \n",
            "_________________________________________________________________\n",
            "batch1 (BatchNormalization)  (None, None, None, 96)    384       \n",
            "_________________________________________________________________\n",
            "act1 (Activation)            (None, None, None, 96)    0         \n",
            "_________________________________________________________________\n",
            "mpool1 (MaxPooling2D)        (None, None, None, 96)    0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, None, None, 256)   614656    \n",
            "_________________________________________________________________\n",
            "batch2 (BatchNormalization)  (None, None, None, 256)   1024      \n",
            "_________________________________________________________________\n",
            "act2 (Activation)            (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "mpool2 (MaxPooling2D)        (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, None, None, 384)   885120    \n",
            "_________________________________________________________________\n",
            "batch3 (BatchNormalization)  (None, None, None, 384)   1536      \n",
            "_________________________________________________________________\n",
            "act3 (Activation)            (None, None, None, 384)   0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, None, None, 256)   884992    \n",
            "_________________________________________________________________\n",
            "batch4 (BatchNormalization)  (None, None, None, 256)   1024      \n",
            "_________________________________________________________________\n",
            "act4 (Activation)            (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "batch5 (BatchNormalization)  (None, None, None, 256)   1024      \n",
            "_________________________________________________________________\n",
            "act5 (Activation)            (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "mpool5 (MaxPooling2D)        (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "fc6 (Conv2D)                 (None, None, None, 2048)  4720640   \n",
            "_________________________________________________________________\n",
            "batch6 (BatchNormalization)  (None, None, None, 2048)  8192      \n",
            "_________________________________________________________________\n",
            "act6 (Activation)            (None, None, None, 2048)  0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "fc7 (Dense)                  (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "batch7 (BatchNormalization)  (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "act7 (Activation)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "fc8 (Dense)                  (None, 1251)              1282275   \n",
            "=================================================================\n",
            "Total params: 11,098,019\n",
            "Trainable params: 11,089,187\n",
            "Non-trainable params: 8,832\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S66ghjND_2MU",
        "colab_type": "text"
      },
      "source": [
        "####compile and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyLhnzyxdOW7",
        "colab_type": "code",
        "outputId": "52dae0b1-4681-42d4-d785-e9b9dd2dc403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import optimizers\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 100,\n",
        "          'n_classes': 1251,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "partition, labels = split_data('train') # IDs & Labels\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition, labels, train=True, **params)\n",
        "\n",
        "partition, labels = split_data('valid') # IDs & Labels\n",
        "validation_generator = DataGenerator(partition, labels, **params)\n",
        "\n",
        "\n",
        "# optimizer = 'sgd'\n",
        "model.compile(optimizer = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9),\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# filepath = \"drive/My\\ Drive/model_vggm.h5\"\n",
        "filepath = \"model_vggm.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "if os.path.isdir('model_vggm.h5'):\n",
        "  model = Models.load_model('model_vggm.h5')\n",
        "  \n",
        "# validation_data=validation_generator,\n",
        "model.fit_generator(generator=training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=10,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=6,\n",
        "                    callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "726/726 [==============================] - 1121s 2s/step - loss: 4.2698 - acc: 0.2142\n",
            "\n",
            "Epoch 00001: loss improved from inf to 4.26980, saving model to model_vggm.h5\n",
            "Epoch 2/10\n",
            "726/726 [==============================] - 1071s 1s/step - loss: 3.7076 - acc: 0.2942\n",
            "\n",
            "Epoch 00002: loss improved from 4.26980 to 3.70760, saving model to model_vggm.h5\n",
            "Epoch 3/10\n",
            "726/726 [==============================] - 1086s 1s/step - loss: 3.2605 - acc: 0.3677\n",
            "\n",
            "Epoch 00003: loss improved from 3.70760 to 3.26047, saving model to model_vggm.h5\n",
            "Epoch 4/10\n",
            "131/726 [====>.........................] - ETA: 15:06 - loss: 3.0143 - acc: 0.4177"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-110:\n",
            "Process ForkPoolWorker-111:\n",
            "Process ForkPoolWorker-114:\n",
            "Process ForkPoolWorker-112:\n",
            "Process ForkPoolWorker-113:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 74, in __data_generation\n",
            "    dither = np.random.uniform(-1, 1, samples.shape)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 96, in __data_generation\n",
            "    mode='magnitude', return_onesided=False)\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 54, in __data_generation\n",
            "    rate, samples = wavfile.read(audio_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 759, in spectrogram\n",
            "    Sxx = np.abs(Sxx)\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 96, in __data_generation\n",
            "    mode='magnitude', return_onesided=False)\n",
            "  File \"<ipython-input-3-8d1ea3f2b63f>\", line 82, in __data_generation\n",
            "    samples = np.append(samples, samples)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 759, in spectrogram\n",
            "    Sxx = np.abs(Sxx)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py\", line 236, in read\n",
            "    file_size, is_big_endian = _read_riff_chunk(fid)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py\", line 4694, in append\n",
            "    return concatenate((arr, values), axis=axis)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/io/wavfile.py\", line 158, in _read_riff_chunk\n",
            "    str1 = fid.read(4)  # File signature\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-46cfb66a155b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     callbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-109:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 125, in worker\n",
            "    put((job, i, result))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 341, in put\n",
            "    obj = _ForkingPickler.dumps(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/reduction.py\", line 51, in dumps\n",
            "    cls(buf, protocol).dump(obj)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqQo-P80I9ao",
        "colab_type": "code",
        "outputId": "743137ca-26ec-4f69-fcbc-2abf24948c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "params = {'dim': (512,298),\n",
        "          'batch_size': 1,\n",
        "          'n_classes': 1251,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "partition, labels = split_data('test') # IDs & Labels\n",
        "test_generator = DataGenerator(partition, labels, train=False, **params)\n",
        "\n",
        "loss, accuracy = model.evaluate_generator(test_generator,\n",
        "                                          use_multiprocessing=True,\n",
        "                                          workers=6)\n",
        "print(loss)\n",
        "print(\"%.2f\" % (accuracy * 100), '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.941980450312297\n",
            "0.53 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt6lJ5AOLWGY",
        "colab_type": "text"
      },
      "source": [
        "####save model to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqtpgvULLVqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.system('cp model_vggm.h5 drive/My\\ Drive/model_vggm.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG728V5198tN",
        "colab_type": "text"
      },
      "source": [
        "###ResNet50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_kmm5jgd3-J",
        "colab_type": "text"
      },
      "source": [
        "####Model Definition - ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YEX2jrXaKzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten,\n",
        "    GlobalAveragePooling2D\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.image_dim_ordering() == 'tf':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        CHANNEL_AXIS = 3\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        if K.image_dim_ordering() == 'tf':\n",
        "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "#         conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
        "        conv1 = _conv_bn_relu(filters=32, kernel_size=(7, 7), strides=(2, 2))(input)\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 32 #64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "#         pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "#                                  strides=(1, 1))(block)\n",
        "        \n",
        "        fc1 = Conv2D(filters=1024, kernel_size=[block_shape[ROW_AXIS],1], strides=[1,1], padding='valid', name='fc1')(block)\n",
        "    \n",
        "#         pool2 = AveragePooling2D(pool_size=[1,block_shape[COL_AXIS]], name='avgpool')(fc1)                \n",
        "#         flatten1 = Flatten()(pool2)\n",
        "        \n",
        "        flatten1 = GlobalAveragePooling2D(name='avgpool')(fc1)\n",
        "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "                      activation=\"softmax\")(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSfJpJd5d8_Q",
        "colab_type": "text"
      },
      "source": [
        "####Build and Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkCjHuT9aRT0",
        "colab_type": "code",
        "outputId": "c86af77d-d1e2-4318-dd5f-50650c83f653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "resnet = ResnetBuilder()\n",
        "model = resnet.build_resnet_50((1,512,298),1251)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 512, 298, 1)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 256, 149, 32) 1600        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 256, 149, 32) 128         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 256, 149, 32) 0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 128, 75, 32)  0           activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 128, 75, 32)  1056        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 128, 75, 32)  128         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 128, 75, 32)  0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 128, 75, 32)  9248        activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 128, 75, 32)  128         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 128, 75, 32)  0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 128, 75, 128) 4224        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 128, 75, 128) 4224        activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 128, 75, 128) 0           conv2d_58[0][0]                  \n",
            "                                                                 conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 128, 75, 128) 512         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 128, 75, 128) 0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 128, 75, 32)  4128        activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 128, 75, 32)  128         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 128, 75, 32)  0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 128, 75, 32)  9248        activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 128, 75, 32)  128         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 128, 75, 32)  0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 128, 75, 128) 4224        activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 128, 75, 128) 0           add_17[0][0]                     \n",
            "                                                                 conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 128, 75, 128) 512         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 128, 75, 128) 0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 128, 75, 32)  4128        activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 128, 75, 32)  128         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 128, 75, 32)  0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 128, 75, 32)  9248        activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 128, 75, 32)  128         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 128, 75, 32)  0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 128, 75, 128) 4224        activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 128, 75, 128) 0           add_18[0][0]                     \n",
            "                                                                 conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 128, 75, 128) 512         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 128, 75, 128) 0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 64, 38, 64)   8256        activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 64, 38, 64)   256         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 64, 38, 64)   0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 64, 38, 64)   36928       activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 64, 38, 64)   256         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 64, 38, 64)   0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 64, 38, 256)  33024       add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 64, 38, 256)  16640       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 64, 38, 256)  0           conv2d_68[0][0]                  \n",
            "                                                                 conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 64, 38, 256)  1024        add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 64, 38, 256)  0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 64, 38, 64)   16448       activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 64, 38, 64)   256         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 64, 38, 64)   0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 64, 38, 64)   36928       activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 64, 38, 64)   256         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 64, 38, 64)   0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 64, 38, 256)  16640       activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 64, 38, 256)  0           add_20[0][0]                     \n",
            "                                                                 conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 64, 38, 256)  1024        add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 64, 38, 256)  0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 64, 38, 64)   16448       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 64, 38, 64)   256         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 64, 38, 64)   0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 64, 38, 64)   36928       activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 64, 38, 64)   256         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 64, 38, 64)   0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 64, 38, 256)  16640       activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 64, 38, 256)  0           add_21[0][0]                     \n",
            "                                                                 conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 64, 38, 256)  1024        add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 64, 38, 256)  0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 64, 38, 64)   16448       activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 64, 38, 64)   256         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 64, 38, 64)   0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 64, 38, 64)   36928       activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 64, 38, 64)   256         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 64, 38, 64)   0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 64, 38, 256)  16640       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 64, 38, 256)  0           add_22[0][0]                     \n",
            "                                                                 conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 64, 38, 256)  1024        add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 64, 38, 256)  0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 32, 19, 128)  32896       activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 32, 19, 128)  512         conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 32, 19, 128)  0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 32, 19, 128)  147584      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 32, 19, 128)  512         conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 32, 19, 128)  0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 32, 19, 512)  131584      add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 32, 19, 512)  66048       activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 32, 19, 512)  0           conv2d_81[0][0]                  \n",
            "                                                                 conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 32, 19, 512)  2048        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 32, 19, 512)  0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 32, 19, 128)  65664       activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 32, 19, 128)  512         conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 32, 19, 128)  0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 32, 19, 128)  147584      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 32, 19, 128)  512         conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 32, 19, 128)  0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 32, 19, 512)  66048       activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 32, 19, 512)  0           add_24[0][0]                     \n",
            "                                                                 conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 32, 19, 512)  2048        add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 32, 19, 512)  0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 32, 19, 128)  65664       activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 32, 19, 128)  512         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 32, 19, 128)  0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 32, 19, 128)  147584      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 32, 19, 128)  512         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 32, 19, 128)  0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 32, 19, 512)  66048       activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 32, 19, 512)  0           add_25[0][0]                     \n",
            "                                                                 conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 32, 19, 512)  2048        add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 32, 19, 512)  0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 32, 19, 128)  65664       activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 32, 19, 128)  512         conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 32, 19, 128)  0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 32, 19, 128)  147584      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 32, 19, 128)  512         conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 32, 19, 128)  0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 32, 19, 512)  66048       activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 32, 19, 512)  0           add_26[0][0]                     \n",
            "                                                                 conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 32, 19, 512)  2048        add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 32, 19, 512)  0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 32, 19, 128)  65664       activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 32, 19, 128)  512         conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 32, 19, 128)  0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 32, 19, 128)  147584      activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 32, 19, 128)  512         conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 32, 19, 128)  0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 32, 19, 512)  66048       activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 32, 19, 512)  0           add_27[0][0]                     \n",
            "                                                                 conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 32, 19, 512)  2048        add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 32, 19, 512)  0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 32, 19, 128)  65664       activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 32, 19, 128)  512         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 32, 19, 128)  0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 32, 19, 128)  147584      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 32, 19, 128)  512         conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 32, 19, 128)  0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 32, 19, 512)  66048       activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 32, 19, 512)  0           add_28[0][0]                     \n",
            "                                                                 conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 32, 19, 512)  2048        add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 32, 19, 512)  0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 16, 10, 256)  131328      activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 16, 10, 256)  1024        conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 16, 10, 256)  0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 16, 10, 256)  590080      activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 16, 10, 256)  1024        conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 16, 10, 256)  0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 16, 10, 1024) 525312      add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 16, 10, 1024) 263168      activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 16, 10, 1024) 0           conv2d_100[0][0]                 \n",
            "                                                                 conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 16, 10, 1024) 4096        add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 16, 10, 1024) 0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 16, 10, 256)  262400      activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 16, 10, 256)  1024        conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 16, 10, 256)  0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 16, 10, 256)  590080      activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 16, 10, 256)  1024        conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 16, 10, 256)  0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 16, 10, 1024) 263168      activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 16, 10, 1024) 0           add_30[0][0]                     \n",
            "                                                                 conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_95 (BatchNo (None, 16, 10, 1024) 4096        add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 16, 10, 1024) 0           batch_normalization_95[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 10, 256)  262400      activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_96 (BatchNo (None, 16, 10, 256)  1024        conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 16, 10, 256)  0           batch_normalization_96[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 10, 256)  590080      activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_97 (BatchNo (None, 16, 10, 256)  1024        conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 16, 10, 256)  0           batch_normalization_97[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 10, 1024) 263168      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 16, 10, 1024) 0           add_31[0][0]                     \n",
            "                                                                 conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_98 (BatchNo (None, 16, 10, 1024) 4096        add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 16, 10, 1024) 0           batch_normalization_98[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "fc1 (Conv2D)                    (None, 1, 10, 1024)  16778240    activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "avgpool (GlobalAveragePooling2D (None, 1024)         0           fc1[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1251)         1282275     avgpool[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,982,179\n",
            "Trainable params: 23,959,459\n",
            "Non-trainable params: 22,720\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydvybnzSABuL",
        "colab_type": "text"
      },
      "source": [
        "####compile and Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F-4fENPoR0r",
        "colab_type": "code",
        "outputId": "30e46f30-2143-4a7e-b76f-66fe19e280e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 64,\n",
        "          'n_classes': 1251,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "partition, labels = split_data('train') # IDs & Labels\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition, labels, train=True, **params)\n",
        "\n",
        "partition, labels = split_data('valid') # IDs & Labels\n",
        "validation_generator = DataGenerator(partition, labels, **params)\n",
        "\n",
        "\n",
        "# optimizer = 'sgd'\n",
        "model.compile(optimizer = 'sgd',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# filepath = \"drive/My\\ Drive/model_resnet50.h5\"\n",
        "filepath = \"model_resnet50.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "# validation_data=validation_generator,\n",
        "model.fit_generator(generator=training_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs=10,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=6,\n",
        "                    callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1080/1080 [==============================] - 1384s 1s/step - loss: 8.7199 - acc: 0.0302 - val_loss: 8.9263 - val_acc: 0.0193\n",
            "\n",
            "Epoch 00001: loss improved from inf to 8.71992, saving model to model_resnet50.h5\n",
            "Epoch 2/10\n",
            "   7/1080 [..............................] - ETA: 49:31 - loss: 8.0270 - acc: 0.0580"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-50:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 405, in _handle_workers\n",
            "    pool._maintain_pool()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 246, in _maintain_pool\n",
            "    self._repopulate_pool()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n",
            "    w.start()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
            "    self._popen = self._Popen(self)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
            "    return Popen(process_obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
            "    self._launch(process_obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n",
            "    self.pid = os.fork()\n",
            "OSError: [Errno 12] Cannot allocate memory\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r   8/1080 [..............................] - ETA: 49:17 - loss: 8.0251 - acc: 0.0605"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-53:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 405, in _handle_workers\n",
            "    pool._maintain_pool()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 246, in _maintain_pool\n",
            "    self._repopulate_pool()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n",
            "    w.start()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
            "    self._popen = self._Popen(self)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
            "    return Popen(process_obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
            "    self._launch(process_obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n",
            "    self.pid = os.fork()\n",
            "OSError: [Errno 12] Cannot allocate memory\n",
            "\n",
            "Process ForkPoolWorker-103:\n",
            "Process ForkPoolWorker-106:\n",
            "Process ForkPoolWorker-105:\n",
            "Process ForkPoolWorker-107:\n",
            "Process ForkPoolWorker-101:\n",
            "Process ForkPoolWorker-98:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Exception ignored in: <Finalize object, dead>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 589, in _terminate_pool\n",
            "    p.terminate()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 116, in terminate\n",
            "    self._popen.terminate()\n",
            "AttributeError: 'NoneType' object has no attribute 'terminate'\n",
            "Exception ignored in: <Finalize object, dead>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/util.py\", line 186, in __call__\n",
            "    res = self._callback(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 589, in _terminate_pool\n",
            "    p.terminate()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 116, in terminate\n",
            "    self._popen.terminate()\n",
            "AttributeError: 'NoneType' object has no attribute 'terminate'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: GPU sync failed",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-338c3279e008>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                     callbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0menqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m                 \u001b[0menqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_enqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfinished_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMTy4v4d97n1",
        "colab_type": "text"
      },
      "source": [
        "##Add Speech Signal Phase to Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZVA0Qxc-ICS",
        "colab_type": "text"
      },
      "source": [
        "###Data Generator (magnitude, phase)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA5okeG7-HTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "class DataGenerator_phase(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, train=True, batch_size=32, dim=(512,300), n_channels=1,\n",
        "                 n_classes=1251, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.train = train\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs[:int(len(list_IDs)/2)]\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        labels_temp = [self.labels[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp, labels_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, audio_path in enumerate(list_IDs_temp):          \n",
        "          # Store class\n",
        "          y[i] = labels_temp[i]\n",
        "\n",
        "          # read .wav\n",
        "          rate, samples = wavfile.read(audio_path)\n",
        "          \n",
        "          ## parameters\n",
        "          window = 'hamming'\n",
        "          # window width and step size\n",
        "          Tw = 25 # ms\n",
        "          Ts = 10 # ms\n",
        "          # frame duration (samples)\n",
        "          Nw = int(rate * Tw * 1e-3)\n",
        "          Ns = int(rate * (Tw - Ts) * 1e-3)\n",
        "          # overlapped duration (samples)\n",
        "          # 2 ** to the next pow of 2 of (Nw - 1)\n",
        "          nfft = 2 ** (Nw - 1).bit_length()\n",
        "          pre_emphasis = 0.97\n",
        "\n",
        "          # preemphasis filter\n",
        "          samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
        "\n",
        "          # removes DC component of the signal and add a small dither\n",
        "          samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
        "          dither = np.random.uniform(-1, 1, samples.shape)\n",
        "          spow = np.std(samples)\n",
        "          samples = samples + 1e-6 * spow * dither\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          for _ in range(2):\n",
        "            samples = np.append(samples, samples)\n",
        "         \n",
        "        \n",
        "        \n",
        "#           if self.train:\n",
        "          # segment selection\n",
        "          segment_len = 3 # sec\n",
        "          upper_bound = len(samples) - segment_len * rate\n",
        "          start = np.random.randint(0, upper_bound)\n",
        "          end = start + segment_len * rate\n",
        "          samples = samples[start:end]\n",
        "\n",
        "          # spectogram\n",
        "          _, _, spec_mag = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                          mode='magnitude', return_onesided=False)\n",
        "\n",
        "          # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
        "          spec_mag *= rate / 10\n",
        "          \n",
        "          spec_mag = Normalize(spec_mag)\n",
        "          spec_mag = ToTensor(spec_mag)\n",
        "          \n",
        "          spec_mag.reshape(spec_mag.shape[0], spec_mag.shape[1], 1)\n",
        "          \n",
        "          # phase\n",
        "          _, _, spec_phase = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                          mode='phase', return_onesided=False)\n",
        "          spec_phase[1:,:] = np.diff(spec_phase, axis=0)\n",
        "          spec_phase *= rate / 10\n",
        "          spec_phase = Normalize(spec_phase)\n",
        "          spec_phase = ToTensor(spec_phase)\n",
        "\n",
        "          spec_phase.reshape(spec_phase.shape[0], spec_phase.shape[1], 1)\n",
        "                    \n",
        "          spec = np.concatenate((spec_mag, spec_phase), axis=2)\n",
        "          \n",
        "          # Store sample\n",
        "          X[i,] = spec\n",
        "        \n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyQQsT5_AIUo",
        "colab_type": "text"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXNt8zDy_jbP",
        "colab_type": "code",
        "outputId": "ec96b3bc-8f2c-4164-e729-30fa615535f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 100,\n",
        "          'n_classes': 1251,\n",
        "          'n_channels': 2,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "partition, labels = split_data('train') # IDs & Labels\n",
        "\n",
        "# Generators\n",
        "training_generator_phase = DataGenerator_phase(partition, labels, train=True, **params)\n",
        "# validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
        "\n",
        "\n",
        "model = vgg_m_builder(num_classes=1251, num_channel=2)\n",
        "\n",
        "# optimizer = 'sgd'\n",
        "model.compile(optimizer = 'sgd',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# validation_data=validation_generator,\n",
        "model.fit_generator(generator=training_generator_phase,\n",
        "                    epochs=10,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "726/726 [==============================] - 3390s 5s/step - loss: 6.2134 - acc: 0.0401\n",
            "Epoch 2/10\n",
            "726/726 [==============================] - 3326s 5s/step - loss: 5.1385 - acc: 0.1065\n",
            "Epoch 3/10\n",
            "726/726 [==============================] - 3392s 5s/step - loss: 4.4520 - acc: 0.1794\n",
            "Epoch 4/10\n",
            "408/726 [===============>..............] - ETA: 24:51 - loss: 3.9853 - acc: 0.2443"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-93f3ef16c186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     workers=6)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[100,96,256,149] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_13/SGD/gradients/mpool1_14/MaxPool_grad/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N07CbwpXxUx4",
        "colab_type": "code",
        "outputId": "a2231bd8-40e0-48ec-cc1b-354aad521b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 100,\n",
        "          'n_classes': 1251,\n",
        "          'n_channels': 2,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "partition, labels = split_data('test') # IDs & Labels\n",
        "\n",
        "# Generators\n",
        "test_generator = DataGenerator_phase(partition, labels, train=False, **params)\n",
        "\n",
        "loss, accuracy = model.evaluate_generator(test_generator,\n",
        "                                          use_multiprocessing=True,\n",
        "                                          workers=6)\n",
        "print(loss)\n",
        "print(\"%.2f\" % (accuracy * 100), '%')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.273668382225967\n",
            "0.17 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3BLgttW9DZm",
        "colab_type": "text"
      },
      "source": [
        "# Pre-Trained Model (Keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_-qyaGJvjw4",
        "colab_type": "text"
      },
      "source": [
        "###Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwUZ939VnXVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a9a73262-00ae-419a-ebb6-a6d64a899705"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np6v9QgEQ0t6",
        "colab_type": "text"
      },
      "source": [
        "###Download Mozilla Cmmon Voice Persian dataset and save to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPl_qabGK_0h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-3/fa.tar.gz\n",
        "! cp fa.tar.gz drive/My\\ Drive/datasets/fa.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWbRl_3XQ8M6",
        "colab_type": "text"
      },
      "source": [
        "### or Import saved dataset from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apjtPrqY0nOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp drive/My\\ Drive/datasets/fa.tar.gz fa.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCaibhQSRvkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir common_voice\n",
        "! tar -C common_voice -xf fa.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrpJ4Of13y4T",
        "colab_type": "text"
      },
      "source": [
        "##as Feature Extractor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ST9OCSDqrSg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/mohsenoon/iust-dl97-project.git\n",
        "! apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0\n",
        "! pip install pyaudio"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik3u2xc_7JhB",
        "colab_type": "text"
      },
      "source": [
        "###for 1194 speakers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RruP61bg1V5D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import collections\n",
        "\n",
        "with open('common_voice/validated.tsv', 'r') as val:\n",
        "  lines = val.readlines()\n",
        "  \n",
        "clients_id = []\n",
        "files_name = []\n",
        "mp3_name = []\n",
        "for x in lines[1:]:\n",
        "  clients_id.append(x.split()[0])\n",
        "  files_name.append(x.split()[1].replace('mp3','wav'))\n",
        "  mp3_name.append(x.split()[1])\n",
        "  \n",
        "import collections\n",
        "sample_per_speaker = 2\n",
        "spk_id = [item for item, count in collections.Counter(clients_id).items() if count >= sample_per_speaker]\n",
        "\n",
        "\n",
        "spk_index = []\n",
        "file_index = []\n",
        "mp3_index = []\n",
        "for i, sid in enumerate(spk_id):\n",
        "  idx = clients_id.index(sid)\n",
        "  [spk_index.append(i) for f in clients_id[idx : idx+sps]]\n",
        "  [file_index.append(os.path.join(DATA_PATH, f)) for f in files_name[idx : idx+sps]]\n",
        "  [mp3_index.append(os.path.join('common_voice/clips', f)) for f in mp3_name[idx : idx+sps]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKvHmqB9LNYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# mp3 to wav\n",
        "if not os.path.isdir('common_voice/wav'):\n",
        "  os.mkdir('common_voice/wav')\n",
        "for i, wav in enumerate(file_index):\n",
        "  os.system('ffmpeg -i {} -ar 16000 {}'.format(mp3_index[i], wav))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daU9GgWNMl3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('iust-dl97-project/cfg/enroll_list.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['filename', 'speaker']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in range(0, len(file_index), 2):\n",
        "      writer.writerow({'filename': '../'+file_index[i], 'speaker': spk_index[i]})\n",
        "\n",
        "with open('iust-dl97-project/cfg/test_list.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['filename', 'speaker']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in range(1, len(file_index), 2):\n",
        "      writer.writerow({'filename': '../'+file_index[i], 'speaker': spk_index[i]})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Cyy2OvrlA0",
        "colab_type": "code",
        "outputId": "62dfe47b-6d81-4b33-c996-c4c788b0cc9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "% cd iust-dl97-project\n",
        "! python scoring.py\n",
        "% cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/iust-dl97-project\n",
            "Using TensorFlow backend.\n",
            "Loading model weights from [model/weights.h5]....\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 14:41:17.353217 140413200598912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 14:41:17.370271 140413200598912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 14:41:17.397982 140413200598912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 14:41:17.398170 140413200598912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0721 14:41:17.398384 140413200598912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2019-07-21 14:41:17.405777: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-21 14:41:17.406050: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x71c08c0 executing computations on platform Host. Devices:\n",
            "2019-07-21 14:41:17.406093: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-21 14:41:17.409098: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-21 14:41:17.515425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:41:17.515999: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x71c0c40 executing computations on platform CUDA. Devices:\n",
            "2019-07-21 14:41:17.516043: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-07-21 14:41:17.516352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:41:17.516760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-21 14:41:17.517167: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 14:41:17.518865: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-21 14:41:17.520389: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-21 14:41:17.520754: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-21 14:41:17.522789: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-21 14:41:17.524356: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-21 14:41:17.529102: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-21 14:41:17.529309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:41:17.529811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:41:17.530213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-21 14:41:17.530290: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 14:41:17.531585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-21 14:41:17.531620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-21 14:41:17.531657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-21 14:41:17.532044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:41:17.532527: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:41:17.532916: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-21 14:41:17.532974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0721 14:41:18.013679 140413200598912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0721 14:41:18.091634 140413200598912 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 512, None, 1)      0         \n",
            "_________________________________________________________________\n",
            "pad1 (ZeroPadding2D)         (None, 514, None, 1)      0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 254, None, 96)     4800      \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 254, None, 96)     384       \n",
            "_________________________________________________________________\n",
            "relu1 (Activation)           (None, 254, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "mpool1 (MaxPooling2D)        (None, 126, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "pad2 (ZeroPadding2D)         (None, 128, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 62, None, 256)     614656    \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 62, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu2 (Activation)           (None, 62, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "mpool2 (MaxPooling2D)        (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "pad3 (ZeroPadding2D)         (None, 32, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 30, None, 384)     885120    \n",
            "_________________________________________________________________\n",
            "bn3 (BatchNormalization)     (None, 30, None, 384)     1536      \n",
            "_________________________________________________________________\n",
            "relu3 (Activation)           (None, 30, None, 384)     0         \n",
            "_________________________________________________________________\n",
            "pad4 (ZeroPadding2D)         (None, 32, None, 384)     0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 30, None, 256)     884992    \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 30, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu4 (Activation)           (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "pad5 (ZeroPadding2D)         (None, 32, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 30, None, 256)     590080    \n",
            "_________________________________________________________________\n",
            "bn5 (BatchNormalization)     (None, 30, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu5 (Activation)           (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "mpool5 (MaxPooling2D)        (None, 9, None, 256)      0         \n",
            "_________________________________________________________________\n",
            "pad6 (ZeroPadding2D)         (None, 9, None, 256)      0         \n",
            "_________________________________________________________________\n",
            "fc6 (Conv2D)                 (None, 1, None, 4096)     9441280   \n",
            "_________________________________________________________________\n",
            "bn6 (BatchNormalization)     (None, 1, None, 4096)     16384     \n",
            "_________________________________________________________________\n",
            "relu6 (Activation)           (None, 1, None, 4096)     0         \n",
            "_________________________________________________________________\n",
            "gapool6 (GlobalAveragePoolin (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape6 (Reshape)           (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "pad7 (ZeroPadding2D)         (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "fc7 (Conv2D)                 (None, 1, 1, 1024)        4195328   \n",
            "_________________________________________________________________\n",
            "bn7 (BatchNormalization)     (None, 1, 1, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "relu7 (Activation)           (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "norm (Lambda)                (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "fc8 (Conv2D)                 (None, 1, 1, 1024)        1049600   \n",
            "=================================================================\n",
            "Total params: 17,691,328\n",
            "Trainable params: 17,678,592\n",
            "Non-trainable params: 12,736\n",
            "_________________________________________________________________\n",
            "Processing enroll samples....\n",
            "2019-07-21 14:48:19.926975: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "Processing test samples....\n",
            "Comparing test samples against enroll samples....\n",
            "Writing outputs to [res/results.csv]....\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N21nTJbQ-m9-"
      },
      "source": [
        "####Accuracy Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3e1a0980-2919-46ea-e2b5-1b9a910afa99",
        "id": "Yv7Yx6k8-m9_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('iust-dl97-project/res/results.csv')\n",
        "\n",
        "accuracy = sum(df['correct']) / len(df['correct'])\n",
        "\n",
        "print('Accuracy =', accuracy * 100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 30.569514237855945 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzr_hqSm7bnK",
        "colab_type": "text"
      },
      "source": [
        "###for 196 speakers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXhSIZEv8uOE",
        "colab_type": "code",
        "outputId": "7e403cee-19e9-4efd-ea77-3beae4370e60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import collections\n",
        "\n",
        "with open('common_voice/validated.tsv', 'r') as val:\n",
        "  lines = val.readlines()\n",
        "  \n",
        "clients_id = []\n",
        "files_name = []\n",
        "mp3_name = []\n",
        "for x in lines[1:]:\n",
        "  clients_id.append(x.split()[0])\n",
        "  files_name.append(x.split()[1].replace('mp3','wav'))\n",
        "  mp3_name.append(x.split()[1])\n",
        "  \n",
        "import collections\n",
        "sample_per_speaker = 40\n",
        "spk_id = [item for item, count in collections.Counter(clients_id).items() if count >= sample_per_speaker]\n",
        "\n",
        "print('number of speakers =', len(spk_id))\n",
        "\n",
        "spk_index = []\n",
        "file_index = []\n",
        "mp3_index = []\n",
        "for i, sid in enumerate(spk_id):\n",
        "  idx = clients_id.index(sid)\n",
        "  [spk_index.append(i) for f in clients_id[idx : idx+sps]]\n",
        "  [file_index.append(os.path.join(DATA_PATH, f)) for f in files_name[idx : idx+sps]]\n",
        "  [mp3_index.append(os.path.join('common_voice/clips', f)) for f in mp3_name[idx : idx+sps]]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of speakers = 196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY6_o0-p7kRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('iust-dl97-project/cfg/enroll_list.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['filename', 'speaker']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in range(0, len(file_index), 2):\n",
        "      writer.writerow({'filename': '../'+file_index[i], 'speaker': spk_index[i]})\n",
        "\n",
        "with open('iust-dl97-project/cfg/test_list.csv', mode='w') as csv_file:\n",
        "    fieldnames = ['filename', 'speaker']\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for i in range(1, len(file_index), 2):\n",
        "      writer.writerow({'filename': '../'+file_index[i], 'speaker': spk_index[i]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGqCR4Ky7neQ",
        "colab_type": "code",
        "outputId": "e8fed2bc-d023-4ba3-c183-d1b04d3f163c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "% cd iust-dl97-project\n",
        "! python scoring.py\n",
        "% cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/iust-dl97-project\n",
            "Using TensorFlow backend.\n",
            "Loading model weights from [model/weights.h5]....\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 14:33:04.827537 140481328191360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0721 14:33:04.844577 140481328191360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0721 14:33:04.872748 140481328191360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0721 14:33:04.872946 140481328191360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0721 14:33:04.873126 140481328191360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2019-07-21 14:33:04.878503: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-21 14:33:04.878760: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x75348c0 executing computations on platform Host. Devices:\n",
            "2019-07-21 14:33:04.878799: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-21 14:33:04.881063: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-07-21 14:33:04.987022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:33:04.987641: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7534c40 executing computations on platform CUDA. Devices:\n",
            "2019-07-21 14:33:04.987675: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-07-21 14:33:04.987943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:33:04.988391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-07-21 14:33:04.988799: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 14:33:04.990374: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-07-21 14:33:04.992101: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-07-21 14:33:04.992504: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-07-21 14:33:04.994347: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-07-21 14:33:04.995823: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-07-21 14:33:05.000030: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-07-21 14:33:05.000178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:33:05.000655: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:33:05.001015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-07-21 14:33:05.001095: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-07-21 14:33:05.002646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-21 14:33:05.002697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-07-21 14:33:05.002715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-07-21 14:33:05.003148: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:33:05.003680: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-21 14:33:05.004053: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-07-21 14:33:05.004114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10802 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "W0721 14:33:05.498905 140481328191360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0721 14:33:05.576960 140481328191360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "2019-07-21 14:33:06.733841: W tensorflow/core/framework/allocator.cc:107] Allocation of 37748736 exceeds 10% of system memory.\n",
            "2019-07-21 14:33:06.733986: W tensorflow/core/framework/allocator.cc:107] Allocation of 16777216 exceeds 10% of system memory.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           (None, 512, None, 1)      0         \n",
            "_________________________________________________________________\n",
            "pad1 (ZeroPadding2D)         (None, 514, None, 1)      0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, 254, None, 96)     4800      \n",
            "_________________________________________________________________\n",
            "bn1 (BatchNormalization)     (None, 254, None, 96)     384       \n",
            "_________________________________________________________________\n",
            "relu1 (Activation)           (None, 254, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "mpool1 (MaxPooling2D)        (None, 126, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "pad2 (ZeroPadding2D)         (None, 128, None, 96)     0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 62, None, 256)     614656    \n",
            "_________________________________________________________________\n",
            "bn2 (BatchNormalization)     (None, 62, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu2 (Activation)           (None, 62, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "mpool2 (MaxPooling2D)        (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "pad3 (ZeroPadding2D)         (None, 32, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 30, None, 384)     885120    \n",
            "_________________________________________________________________\n",
            "bn3 (BatchNormalization)     (None, 30, None, 384)     1536      \n",
            "_________________________________________________________________\n",
            "relu3 (Activation)           (None, 30, None, 384)     0         \n",
            "_________________________________________________________________\n",
            "pad4 (ZeroPadding2D)         (None, 32, None, 384)     0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 30, None, 256)     884992    \n",
            "_________________________________________________________________\n",
            "bn4 (BatchNormalization)     (None, 30, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu4 (Activation)           (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "pad5 (ZeroPadding2D)         (None, 32, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 30, None, 256)     590080    \n",
            "_________________________________________________________________\n",
            "bn5 (BatchNormalization)     (None, 30, None, 256)     1024      \n",
            "_________________________________________________________________\n",
            "relu5 (Activation)           (None, 30, None, 256)     0         \n",
            "_________________________________________________________________\n",
            "mpool5 (MaxPooling2D)        (None, 9, None, 256)      0         \n",
            "_________________________________________________________________\n",
            "pad6 (ZeroPadding2D)         (None, 9, None, 256)      0         \n",
            "_________________________________________________________________\n",
            "fc6 (Conv2D)                 (None, 1, None, 4096)     9441280   \n",
            "_________________________________________________________________\n",
            "bn6 (BatchNormalization)     (None, 1, None, 4096)     16384     \n",
            "_________________________________________________________________\n",
            "relu6 (Activation)           (None, 1, None, 4096)     0         \n",
            "_________________________________________________________________\n",
            "gapool6 (GlobalAveragePoolin (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "reshape6 (Reshape)           (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "pad7 (ZeroPadding2D)         (None, 1, 1, 4096)        0         \n",
            "_________________________________________________________________\n",
            "fc7 (Conv2D)                 (None, 1, 1, 1024)        4195328   \n",
            "_________________________________________________________________\n",
            "bn7 (BatchNormalization)     (None, 1, 1, 1024)        4096      \n",
            "_________________________________________________________________\n",
            "relu7 (Activation)           (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "norm (Lambda)                (None, 1, 1, 1024)        0         \n",
            "_________________________________________________________________\n",
            "fc8 (Conv2D)                 (None, 1, 1, 1024)        1049600   \n",
            "=================================================================\n",
            "Total params: 17,691,328\n",
            "Trainable params: 17,678,592\n",
            "Non-trainable params: 12,736\n",
            "_________________________________________________________________\n",
            "Processing enroll samples....\n",
            "2019-07-21 14:33:50.492774: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "Processing test samples....\n",
            "Comparing test samples against enroll samples....\n",
            "Writing outputs to [res/results.csv]....\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWdrSB259WPv",
        "colab_type": "text"
      },
      "source": [
        "####Accuracy Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om-9I0zd9Rbh",
        "colab_type": "code",
        "outputId": "adc2c299-281e-447a-c564-717eff111372",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('iust-dl97-project/res/results.csv')\n",
        "\n",
        "accuracy = sum(df['correct']) / len(df['correct'])\n",
        "\n",
        "print('Accuracy =', accuracy * 100, '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 50.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1zyvd83nnsZ",
        "colab_type": "text"
      },
      "source": [
        "##FineTuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFRo5cIqnmWg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "03549669-24eb-4b47-f7e5-3b09d095c710"
      },
      "source": [
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/models/vggvox_ident_net.mat"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-25 06:18:00--  http://www.robots.ox.ac.uk/~vgg/data/voxceleb/models/vggvox_ident_net.mat\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66806091 (64M)\n",
            "Saving to: ‘vggvox_ident_net.mat’\n",
            "\n",
            "vggvox_ident_net.ma 100%[===================>]  63.71M  4.11MB/s    in 15s     \n",
            "\n",
            "2019-07-25 06:18:16 (4.17 MB/s) - ‘vggvox_ident_net.mat’ saved [66806091/66806091]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DgS-Qv3x3tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.io import loadmat\n",
        "import numpy as np\n",
        "\n",
        "net = loadmat('vggvox_ident_net.mat',\n",
        "                matlab_compatible=False,\n",
        "                struct_as_record=False)\n",
        "net = net['net'][0,0]\n",
        "layers = net.layers[0]\n",
        "\n",
        "layers_dict = {}\n",
        "for layer in layers:\n",
        "  layers_dict[layer[0,0].name[0]] = layer[0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpTOf7ewzFbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('common_voice/validated.tsv', 'r') as val:\n",
        "  lines = val.readlines()\n",
        "  \n",
        "clients_id = []\n",
        "files_name = []\n",
        "for x in lines[1:]:\n",
        "  clients_id.append(x.split()[0])\n",
        "  files_name.append(x.split()[1].replace('mp3','wav'))\n",
        "  \n",
        "import collections\n",
        "sps = 40\n",
        "spk_id = [item for item, count in collections.Counter(clients_id).items() if count >= sps]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ1hjNG6zkI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = 'common_voice/wav'\n",
        "\n",
        "spk_index = []\n",
        "file_index = []\n",
        "for i, sid in enumerate(spk_id):\n",
        "  idx = clients_id.index(sid)\n",
        "  [spk_index.append(i) for f in clients_id[idx : idx+sps]]\n",
        "  [file_index.append(os.path.join(DATA_PATH, f)) for f in files_name[idx : idx+sps]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aswb9GmQzabj",
        "colab_type": "text"
      },
      "source": [
        "###convert mp3 to wav"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1mBb43FzTfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "# mp3 to wav\n",
        "mp3_path = 'common_voice/clips'\n",
        "wav_path = 'common_voice/wav'\n",
        "for wav in file_index:\n",
        "  os.system('ffmpeg -i {}.mp3 -ar 16000 {}'.format(os.path.join(mp3_path, os.path.splitext(wav)[0]), os.path.join(wav_path, wav)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JelWmvr2yeM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Model\n",
        "from keras.layers import Conv2D, Dense, Flatten, Activation, Input, Reshape\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D, ZeroPadding2D, GlobalAveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def vgg_m_builder_4096(num_classes, num_channel):\n",
        "  \n",
        "  inputs = Input(shape=(None,None,num_channel), name='input')\n",
        "\n",
        "  x = Conv2D(96, 7, strides=2, padding='same', name='conv1')(inputs)\n",
        "  x = BatchNormalization(trainable=False, name='batch1')(x)\n",
        "  x = Activation('relu', name='act1')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool1')(x)\n",
        "  \n",
        "  x = Conv2D(256, 5, strides=2, padding='same', name='conv2')(x)\n",
        "  x = BatchNormalization(name='batch2')(x)\n",
        "  x = Activation('relu', name='act2')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool2')(x)\n",
        "  \n",
        "  x = Conv2D(384, 3, strides=1, padding='same', name='conv3')(x)\n",
        "  x = BatchNormalization(name='batch3')(x)\n",
        "  x = Activation('relu', name='act3')(x)\n",
        "  \n",
        "  x = Conv2D(256, 3, strides=1, padding='same', name='conv4')(x)\n",
        "  x = BatchNormalization(name='batch4')(x)\n",
        "  x = Activation('relu', name='act4')(x)\n",
        "  \n",
        "  x = Conv2D(256, 3, strides=1, padding='same', name='conv5')(x)\n",
        "  x = BatchNormalization(name='batch5')(x)\n",
        "  x = Activation('relu', name='act5')(x)\n",
        "  x = MaxPooling2D(pool_size=(5,3), strides=(3,2), name='mpool5')(x)\n",
        "  \n",
        "  x = Conv2D(4096, (9,1), strides=1, padding='valid', name='fc6')(x)\n",
        "  x = BatchNormalization(name='batch6')(x)\n",
        "  x = Activation('relu', name='act6')(x)\n",
        "#   x = AveragePooling2D(pool_size=(1,int(x.shape[2])), strides=1, name='apool6')(x)\n",
        "#   x = Flatten(name='flat1')(x)\n",
        "\n",
        "  x = GlobalAveragePooling2D()(x)\n",
        "  \n",
        "#   x = Reshape((1,1,4096))(x)\n",
        "  \n",
        "  x = Dense(1024, name='fc7')(x)\n",
        "  x = BatchNormalization(name='batch7')(x)\n",
        "  x = Activation('relu', name='act7')(x)    \n",
        "  \n",
        "  predictions = Dense(num_classes, activation='softmax', name='fc8')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlrfwomkyGzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = vgg_m_builder_4096(1251, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jayK86wdx55u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "fb0e8ca2-16af-4825-e8d7-462e7573fd2c"
      },
      "source": [
        "layers_name = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6', 'fc7', 'fc8']\n",
        "for name in layers_name:\n",
        "  weights =[]\n",
        "  weights.append(layers_dict[name].weights[0,0])\n",
        "  weights.append(np.asarray([b[0] for b in layers_dict[name].weights[0,1]]))\n",
        "  model.get_layer(name).set_weights(weights)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c801c7ba8a5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1055\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m                                  \u001b[0;34m' not compatible with '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m                                  'provided weight shape ' + str(w.shape))\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer weight shape (4096, 1024) not compatible with provided weight shape (1, 1, 4096, 1024)"
          ]
        }
      ]
    }
  ]
}