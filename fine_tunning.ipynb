{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fine_tunning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEGyvsiqSE0U",
        "colab_type": "text"
      },
      "source": [
        "##Download and Extract VoxCeleb1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5ZqmIcGODKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\n",
        "\n",
        "! wget --user voxceleb1902 --password nx0bl2v2 http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\n",
        "\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data/vox1_dev_txt.zip  \n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/data/vox1_test_txt.zip\n",
        "\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/iden_split.txt\n",
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test.txt\n",
        "  \n",
        "! cat vox1_dev* > vox1_dev_wav.zip\n",
        "! rm vox1_dev_wav_partaa vox1_dev_wav_partab vox1_dev_wav_partac vox1_dev_wav_partad\n",
        "! mkdir -p voxceleb1\n",
        "! mv *.zip voxceleb1\n",
        "! mv *.txt voxceleb1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW7bg-7mSZYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "DATA_PATH = 'voxceleb1/'\n",
        "\n",
        "print('Starting to unpack vox1_dev_wav.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_dev_wav.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_test_wav.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_test_wav.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_dev_txt.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_dev_txt.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done. Starting to unpack vox1_test_txt.zip')\n",
        "zip = zipfile.ZipFile(os.path.join(DATA_PATH, 'vox1_test_txt.zip'), 'r')\n",
        "zip.extractall(DATA_PATH)\n",
        "zip.close()\n",
        "print('Done.')\n",
        "\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_dev_wav.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_test_wav.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_dev_txt.zip'))\n",
        "os.remove(os.path.join(DATA_PATH, 'vox1_test_txt.zip'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz0gpr1uqaXh",
        "colab_type": "text"
      },
      "source": [
        "##with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdIw3iYlsk24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import wavfile\n",
        "from scipy import signal\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # scipy throws future warnings on fft (known bug)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3TUYV9hUbFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Normalize(spec):\n",
        "  \"\"\"Normalizes voice spectrogram (mean-varience)\"\"\"\n",
        "  # (Freq, Time)\n",
        "  # mean-variance normalization for every spectrogram (not batch-wise)\n",
        "  mu = spec.mean(axis=1).reshape(spec.shape[0], 1)\n",
        "  sigma = spec.std(axis=1).reshape(spec.shape[0], 1)\n",
        "  spec = (spec - mu) / sigma\n",
        "\n",
        "  return spec\n",
        "\n",
        "def ToTensor(spec):\n",
        "  \"\"\"Convert spectogram to Tensor.\"\"\"\n",
        "  F, T = spec.shape\n",
        "\n",
        "  # now specs are of size (Freq, Time) and 2D but has to be 3D (channel dim)\n",
        "  spec = spec.reshape(F, T, 1)\n",
        "\n",
        "  # make the ndarray to be of a proper type (was float64)\n",
        "  spec = spec.astype(np.float32)\n",
        "\n",
        "#   return torch.from_numpy(spec)\n",
        "  return spec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laqMUYgBM0xW",
        "colab_type": "text"
      },
      "source": [
        "####Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK5sxmZsLnGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, train=True, batch_size=32, dim=(512,300), n_channels=1,\n",
        "                 n_classes=1251, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.train = train\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        labels_temp = [self.labels[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp, labels_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, audio_path in enumerate(list_IDs_temp):          \n",
        "          # Store class\n",
        "          y[i] = labels_temp[i]\n",
        "\n",
        "          # read .wav\n",
        "          rate, samples = wavfile.read(audio_path)\n",
        "          \n",
        "          ## parameters\n",
        "          window = 'hamming'\n",
        "          # window width and step size\n",
        "          Tw = 25 # ms\n",
        "          Ts = 10 # ms\n",
        "          # frame duration (samples)\n",
        "          Nw = int(rate * Tw * 1e-3)\n",
        "          Ns = int(rate * (Tw - Ts) * 1e-3)\n",
        "          # overlapped duration (samples)\n",
        "          # 2 ** to the next pow of 2 of (Nw - 1)\n",
        "          nfft = 2 ** (Nw - 1).bit_length()\n",
        "          pre_emphasis = 0.97\n",
        "\n",
        "          # preemphasis filter\n",
        "          samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
        "\n",
        "          # removes DC component of the signal and add a small dither\n",
        "          samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
        "          dither = np.random.uniform(-1, 1, samples.shape)\n",
        "          spow = np.std(samples)\n",
        "          samples = samples + 1e-6 * spow * dither\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          for _ in range(2):\n",
        "            samples = np.append(samples, samples)\n",
        "         \n",
        "        \n",
        "        \n",
        "          if self.train:\n",
        "              # segment selection\n",
        "              segment_len = 3 # sec\n",
        "              upper_bound = len(samples) - segment_len * rate\n",
        "              start = np.random.randint(0, upper_bound)\n",
        "              end = start + segment_len * rate\n",
        "              samples = samples[start:end]\n",
        "\n",
        "          # spectogram\n",
        "          _, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                          mode='complex', return_onesided=False)\n",
        "\n",
        "          # just multiplying it by 1600 makes spectrograms in the paper and here \"the same\"\n",
        "          spec *= rate / 10\n",
        "          \n",
        "          spec = Normalize(spec)\n",
        "          spec = ToTensor(spec)\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "                                       \n",
        "          _, _, spec_phase = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                                mode='phase', return_onesided=False)\n",
        "          spec_ = np.concatenate((spec, np.expand_dims(spec_phase, axis=-1)), axis=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          \n",
        "          # Store sample\n",
        "          X[i,] = spec\n",
        "        \n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUsg1BFMN4a7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import signal\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4tXRJ1iOCYX",
        "colab_type": "code",
        "outputId": "6e390399-76b9-4db4-9663-fdea9283ba9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# read .wav\n",
        "rate, samples = wavfile.read('madc0_si737.wav')\n",
        "\n",
        "## parameters\n",
        "window = 'hamming'\n",
        "# window width and step size\n",
        "Tw = 25 # ms\n",
        "Ts = 10 # ms\n",
        "# frame duration (samples)\n",
        "Nw = int(rate * Tw * 1e-3)\n",
        "Ns = int(rate * (Tw - Ts) * 1e-3)\n",
        "# overlapped duration (samples)\n",
        "# 2 ** to the next pow of 2 of (Nw - 1)\n",
        "nfft = 2 ** (Nw - 1).bit_length()\n",
        "pre_emphasis = 0.97\n",
        "\n",
        "# preemphasis filter\n",
        "samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
        "\n",
        "# removes DC component of the signal and add a small dither\n",
        "samples = signal.lfilter([1, -1], [1, -0.99], samples)\n",
        "dither = np.random.uniform(-1, 1, samples.shape)\n",
        "spow = np.std(samples)\n",
        "samples = samples + 1e-6 * spow * dither\n",
        "\n",
        "_, _, spec = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                          mode='magnitude', return_onesided=False)\n",
        "\n",
        "# cv2_imshow(spec)\n",
        "                                \n",
        "_, _, spec_phase = signal.spectrogram(samples, rate, window, Nw, Ns, nfft, \n",
        "                                      mode='phase', return_onesided=False)\n",
        "\n",
        "# cv2_imshow(spec_phase)\n",
        "\n",
        "spec = np.concatenate((np.expand_dims(spec, axis=-1), np.expand_dims(spec_phase, axis=-1)), axis=2)\n",
        "\n",
        "spec.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 478, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV4B6h_pMimi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_data(phase):\n",
        "  iden_split_path = os.path.join(DATA_PATH, 'iden_split.txt')\n",
        "  split = pd.read_table(iden_split_path, sep=' ', header=None, names=['phase', 'path'])\n",
        "\n",
        "  if phase == 'train':\n",
        "    phases = [1, 2]\n",
        "  else:\n",
        "    phases = [3]\n",
        "\n",
        "  mask = split['phase'].isin(phases)\n",
        "\n",
        "  dataset = split['path'][mask].reset_index(drop=True)\n",
        "  path = DATA_PATH\n",
        "\n",
        "  list_IDs = [os.path.join(DATA_PATH, 'wav', track_path) for track_path in dataset]\n",
        "  labels = [int(track_path.split('/')[0].replace('id1', '')) - 1 for track_path in dataset]\n",
        "  \n",
        "  return list_IDs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxOrEet2MlQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 100,\n",
        "          'n_classes': 1251,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "partition, labels = split_data('train') # IDs & Labels\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition, labels, train=True, **params)\n",
        "# validation_generator = DataGenerator(partition['validation'], labels, **params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pTuJmzGquAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Model\n",
        "from keras.layers import Conv2D, Dense, Flatten, Activation, Input\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def my_model(num_classes):\n",
        "  \n",
        "  inputs = Input(shape=(512,298,1), name='input')\n",
        "\n",
        "  x = ZeroPadding2D(1, name='pad1')(inputs)\n",
        "  x = Conv2D(96, 7, strides=2, name='conv1')(x)\n",
        "  x = BatchNormalization(trainable=False, name='batch1')(x)\n",
        "  x = Activation('relu', name='act1')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool1')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad2')(x)\n",
        "  x = Conv2D(256, 5, strides=2, name='conv2')(x)\n",
        "  x = BatchNormalization(name='batch2')(x)\n",
        "  x = Activation('relu', name='act2')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool2')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad3')(x)\n",
        "  x = Conv2D(384, 3, strides=1, name='conv3')(x)\n",
        "  x = BatchNormalization(name='batch3')(x)\n",
        "  x = Activation('relu', name='act3')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad4')(x)\n",
        "  x = Conv2D(256, 3, strides=1, name='conv4')(x)\n",
        "  x = BatchNormalization(name='batch4')(x)\n",
        "  x = Activation('relu', name='act4')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad5')(x)\n",
        "  x = Conv2D(256, 3, strides=1, name='conv5')(x)\n",
        "  x = BatchNormalization(name='batch5')(x)\n",
        "  x = Activation('relu', name='act5')(x)\n",
        "  x = MaxPooling2D(pool_size=(5,3), strides=(3,2), name='mpool5')(x)\n",
        "  \n",
        "  x = Conv2D(4096, (9,1), strides=1, name='fc6')(x)\n",
        "  x = BatchNormalization(name='batch6')(x)\n",
        "  x = Activation('relu', name='act6')(x)\n",
        "  x = AveragePooling2D(pool_size=(1,int(x.shape[2])), strides=1, name='apool6')(x)\n",
        "  \n",
        "  x = Flatten(name='flat1')(x)\n",
        "  \n",
        "  x = Dense(1024, name='fc7')(x)\n",
        "  x = BatchNormalization(name='batch7')(x)\n",
        "  x = Activation('relu', name='act7')(x)    \n",
        "  \n",
        "  predictions = Dense(num_classes, activation='softmax', name='fc8')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAkqIR_X0AGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = my_model(1251)\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWNRSr89YpHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer = 'sgd'\n",
        "model.compile(optimizer = 'sgd',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfEanO3Whuf5",
        "colab_type": "code",
        "outputId": "95e69413-1482-4714-e592-add7edbe0e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# validation_data=validation_generator,\n",
        "model.fit_generator(generator=training_generator,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=6)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            " 528/1452 [=========>....................] - ETA: 45:17 - loss: 6.9389 - acc: 0.0140"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-125:\n",
            "Process ForkPoolWorker-124:\n",
            "Process ForkPoolWorker-126:\n",
            "Process ForkPoolWorker-122:\n",
            "Process ForkPoolWorker-121:\n",
            "Process ForkPoolWorker-123:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 112, in __data_generation\n",
            "    spec_ = np.concatenate((spec, np.expand_dims(spec_phase, axis=-1)), axis=2)\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 70, in __data_generation\n",
            "    samples = np.append(samples[0], samples[1:] - pre_emphasis * samples[:-1])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 401, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 75, in __data_generation\n",
            "    spow = np.std(samples)\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 32, in __getitem__\n",
            "    X, y = self.__data_generation(list_IDs_temp, labels_temp)\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 96, in __data_generation\n",
            "    mode='complex', return_onesided=False)\n",
            "KeyboardInterrupt\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 96, in __data_generation\n",
            "    mode='complex', return_onesided=False)\n",
            "  File \"<ipython-input-96-332edff2500b>\", line 96, in __data_generation\n",
            "    mode='complex', return_onesided=False)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 756, in spectrogram\n",
            "    mode='stft')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\", line 3242, in std\n",
            "    **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 756, in spectrogram\n",
            "    mode='stft')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 756, in spectrogram\n",
            "    mode='stft')\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 1859, in _spectral_helper\n",
            "    result = result.astype(outdtype)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 140, in _std\n",
            "    keepdims=keepdims)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 1836, in _spectral_helper\n",
            "    result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft, sides)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 1836, in _spectral_helper\n",
            "    result = _fft_helper(x, win, detrend_func, nperseg, noverlap, nfft, sides)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\", line 117, in _var\n",
            "    x = asanyarray(arr - arrmean)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 1910, in _fft_helper\n",
            "    result = detrend_func(result)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 1913, in _fft_helper\n",
            "    result = win * result\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py\", line 1791, in detrend_func\n",
            "    return signaltools.detrend(d, type=detrend, axis=-1)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/signal/signaltools.py\", line 2554, in detrend\n",
            "    ret = data - expand_dims(mean(data, axis), axis)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-c06b0ac89e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit_generator(generator=training_generator,\n\u001b[1;32m      2\u001b[0m                     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     workers=6)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc1Sz76V1Pbh",
        "colab_type": "text"
      },
      "source": [
        " 815/1452 [===============>..............] - ETA: 31:22 - loss: 6.6341 - acc: 0.0267\n",
        " \n",
        "  467/1452 [========>.....................] - ETA: 29:15 - loss: 6.8480 - acc: 0.0172"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ioYnz0HTeD",
        "colab_type": "text"
      },
      "source": [
        "####Pre-Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIQofClsI1I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget http://www.robots.ox.ac.uk/~vgg/data/voxceleb/models/vggvox_ident_net.mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg9khcv1WFnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "net = loadmat('vggvox_ident_net.mat',\n",
        "                matlab_compatible=False,\n",
        "                struct_as_record=False)\n",
        "net = net['net'][0,0]\n",
        "layers = net.layers[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umU3cPmJYDIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers_dict = {}\n",
        "for layer in layers:\n",
        "  layers_dict[layer[0,0].name[0]] = layer[0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-REgaTmdd9S1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# layers_name = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6', 'fc7', 'fc8']\n",
        "# for name in layers_name:\n",
        "#   weights =[]\n",
        "#   weights.append(layers_dict[name].weights[0,0])\n",
        "#   weights.append(np.asarray([b[0] for b in layers_dict[name].weights[0,1]]))\n",
        "#   model.get_layer(name).set_weights(weights)\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_JJuXwshLnk",
        "colab_type": "text"
      },
      "source": [
        "####Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfoQJ0ahhKvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 1,\n",
        "          'n_classes': 1251,\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Datasets\n",
        "test_set, test_labels = split_data('test') # IDs & Labels\n",
        "\n",
        "# Generators\n",
        "test_generator = DataGenerator(test_set, test_labels, train=False, **params)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDpjNkxfhZNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, accuracy = model.evaluate_generator(test_generator)\n",
        "print(loss, accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGgx-UDWmslt",
        "colab_type": "text"
      },
      "source": [
        "###Common Voice dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x8xD7nMm9wN",
        "colab_type": "text"
      },
      "source": [
        "####Download Persian dataset and save to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM690J6ZnEtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! wget https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-3/fa.tar.gz\n",
        "# ! mv fa.tar.gz drive/My\\ Drive/datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hAaRUncmz9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-xQ06S_uQTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp drive/My\\ Drive/datasets/fa.tar.gz fa.tar.gz\n",
        "! mkdir common_voice\n",
        "! tar -C common_voice -xf fa.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDnmA2s5xzBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('common_voice/validated.tsv', 'r') as val:\n",
        "  lines = val.readlines()\n",
        "  \n",
        "clients_id = []\n",
        "files_name = []\n",
        "for x in lines[1:]:\n",
        "  clients_id.append(x.split()[0])\n",
        "  files_name.append(x.split()[1].replace('mp3','wav'))\n",
        "  \n",
        "import collections\n",
        "sps = 40\n",
        "spk_id = [item for item, count in collections.Counter(clients_id).items() if count >= sps]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qql3k4PXIccz",
        "colab_type": "code",
        "outputId": "28f70dce-8d52-4670-91aa-1eb8e017c984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(spk_id))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIKSya5ry2Mu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = 'common_voice/wav'\n",
        "\n",
        "spk_index = []\n",
        "file_index = []\n",
        "for i, sid in enumerate(spk_id):\n",
        "  idx = clients_id.index(sid)\n",
        "  [spk_index.append(i) for f in clients_id[idx : idx+sps]]\n",
        "  [file_index.append(os.path.join(DATA_PATH, f)) for f in files_name[idx : idx+sps]]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSz0cVp2MCjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# # mp3 to wav\n",
        "# mp3_path = 'common_voice/clips'\n",
        "# wav_path = 'common_voice/wav'\n",
        "# for wav in file_index:\n",
        "#   os.system('ffmpeg -i {}.mp3 -ar 16000 {}'.format(os.path.join(mp3_path, os.path.splitext(wav)[0]), os.path.join(wav_path, wav)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT9TX0_51twf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "indexes = np.arange(len(spk_index))\n",
        "np.random.shuffle(indexes)\n",
        "\n",
        "train_set = [file_index[i] for i in indexes[:int(70*len(spk_index)/100)]]\n",
        "train_labels = [spk_index[i] for i in indexes[:int(70*len(spk_index)/100)]]\n",
        "\n",
        "valid_set = [file_index[i] for i in indexes[int(70*len(spk_index)/100) : int(80*len(spk_index)/100)]]\n",
        "valid_labels = [spk_index[i] for i in indexes[int(70*len(spk_index)/100) : int(80*len(spk_index)/100)]]\n",
        "\n",
        "test_set = [file_index[i] for i in indexes[int(80*len(spk_index)/100):]]\n",
        "test_labels = [spk_index[i] for i in indexes[int(80*len(spk_index)/100):]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBtcEg66aMPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[print(f, file=open('train_labels.txt','a')) for f in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVT-mBKBQc0y",
        "colab_type": "code",
        "outputId": "2bd6506b-6916-4ab9-960d-c2734534e239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 100,\n",
        "          'n_classes': len(spk_id),\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "test_generator = DataGenerator(test_set, test_labels, train=True, **params)\n",
        "# validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
        "\n",
        "loss, accuracy = cv_model.evaluate_generator(test_generator)\n",
        "print(loss)\n",
        "print(\"%.2f\" % (accuracy * 100), '%')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.361034297943116\n",
            "0.20 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3H2WKLvZz4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_model = my_model_drop(len(spk_id))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu7vY995aN3a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers_name = ['conv1', 'conv2', 'conv3', 'conv4', 'conv5', 'fc6']\n",
        "for name in layers_name:\n",
        "  weights =[]\n",
        "  weights.append(layers_dict[name].weights[0,0])\n",
        "  weights.append(np.asarray([b[0] for b in layers_dict[name].weights[0,1]]))\n",
        "  cv_model.get_layer(name).set_weights(weights)\n",
        "  cv_model.get_layer(name).trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYWWKENrzWx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "params = {'dim': (512,298),\n",
        "          'batch_size': 100,\n",
        "          'n_classes': len(spk_id),\n",
        "          'n_channels': 1,\n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "train_generator = DataGenerator(train_set, train_labels, train=True, **params)\n",
        "validation_generator = DataGenerator(valid_set, valid_labels, train=True, **params)\n",
        "\n",
        "# optimizer = 'sgd' 'rmsprob' 'adam'\n",
        "cv_model.compile(optimizer = 'adam',\n",
        "              loss = 'categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqXBl3h3cjh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation_data=validation_generator,\n",
        "cv_model.fit_generator(generator=train_generator,\n",
        "                       validation_data=validation_generator,\n",
        "                       epochs = 30,\n",
        "                       use_multiprocessing=True,\n",
        "                       workers=6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i876_QxUo4bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Model\n",
        "from keras.layers import Conv2D, Dense, Flatten, Activation, Input, Dropout\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "def my_model_drop(num_classes):\n",
        "  \n",
        "  inputs = Input(shape=(512,298,1), name='input')\n",
        "\n",
        "  x = ZeroPadding2D(1, name='pad1')(inputs)\n",
        "  x = Conv2D(96, 7, strides=2, name='conv1')(x)\n",
        "  x = BatchNormalization(trainable=False, name='batch1')(x)\n",
        "  x = Activation('relu', name='act1')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool1')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad2')(x)\n",
        "  x = Conv2D(256, 5, strides=2, name='conv2')(x)\n",
        "  x = BatchNormalization(name='batch2')(x)\n",
        "  x = Activation('relu', name='act2')(x)\n",
        "  x = MaxPooling2D(3, 2, name='mpool2')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad3')(x)\n",
        "  x = Conv2D(384, 3, strides=1, name='conv3')(x)\n",
        "  x = BatchNormalization(name='batch3')(x)\n",
        "  x = Activation('relu', name='act3')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad4')(x)\n",
        "  x = Conv2D(256, 3, strides=1, name='conv4')(x)\n",
        "  x = BatchNormalization(name='batch4')(x)\n",
        "  x = Activation('relu', name='act4')(x)\n",
        "  \n",
        "  x = ZeroPadding2D(1, name='pad5')(x)\n",
        "  x = Conv2D(256, 3, strides=1, name='conv5')(x)\n",
        "  x = BatchNormalization(name='batch5')(x)\n",
        "  x = Activation('relu', name='act5')(x)\n",
        "  x = MaxPooling2D(pool_size=(5,3), strides=(3,2), name='mpool5')(x)\n",
        "  \n",
        "  x = Conv2D(4096, (9,1), strides=1, name='fc6')(x)\n",
        "  x = BatchNormalization(name='batch6')(x)\n",
        "  x = Activation('relu', name='act6')(x)\n",
        "  x = AveragePooling2D(pool_size=(1,int(x.shape[2])), strides=1, name='apool6')(x)\n",
        "  \n",
        "  x = Flatten(name='flat1')(x)\n",
        "  \n",
        "  x = Dense(1024, name='fc7',)(x)\n",
        "  x = BatchNormalization(name='batch7')(x)\n",
        "  x = Activation('relu', name='act7')(x)  \n",
        "#   x = Dropout(0.5)(x)\n",
        "  \n",
        "  x = Dense(512, name='fc71')(x)\n",
        "  x = BatchNormalization(name='batch71')(x)\n",
        "  x = Activation('relu', name='act71')(x)  \n",
        "#   x = Dropout(0.5)(x)\n",
        "  \n",
        "  predictions = Dense(num_classes, activation='softmax', name='fc8')(x)\n",
        "\n",
        "  model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}